{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Group Assignment & Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__You should be able to start up on this exercise after Lecture 1.__\n",
    "\n",
    "*This exercise must be a group effort. That means everyone must participate in the assignment.*\n",
    "\n",
    "In this assignment you will solve a data science problem end-to-end, pretending to be recently hired data scientists in\n",
    "a company. To help you get started, we've prepared a checklist to guide you through the project. Here are the main steps\n",
    "that you will go through:\n",
    "\n",
    "1. Frame the problem and look at the big picture\n",
    "2. Get the data\n",
    "3. Explore and visualise the data to gain insights\n",
    "4. Prepare the data to better expose the underlying data patterns to machine learning algorithms\n",
    "5. Explore many different models and short-list the best ones\n",
    "6. Fine-tune your models\n",
    "7. Present your solution (video presentation) \n",
    "\n",
    "In each step we list a set of questions that one should have in mind when undertaking a data science project. The list\n",
    "is not meant to be exhaustive, but does contain a selection of the most important questions to ask. We will be available\n",
    "to provide assistance with each of the steps, and will allocate some part of each lesson towards working on the projects.\n",
    "\n",
    "Your group must submit a _**single**_ Jupyter notebook, structured in terms of the first 6 sections listed above\n",
    "(the seventh will be a video uploaded to some streaming platform, e.g. YouTube, Vimeo, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:39.812426Z",
     "start_time": "2024-12-15T02:02:35.872438Z"
    }
   },
   "source": [
    "# Importing the necessary libraries for this project\n",
    "# Standard Library modules\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import statistics\n",
    "\n",
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:02:36.738188: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-15 03:02:36.770787: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-15 03:02:36.784046: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 03:02:36.943790: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The following is the assignment description for the followning section, hidden from display\n",
    "### 1. Analysis: Frame the problem and look at the big picture\n",
    "1. Find a problem/task that everyone in the group finds interesting\n",
    "2. Define the objective in business terms\n",
    "3. How should you frame the problem (supervised/unsupervised etc.)?\n",
    "4. How should performance be measured?\n",
    "-->\n",
    "## 1. Analysis: Frame the problem and look at the big picture\n",
    "[//] # TODO: Write a description of the problem and the business objective. Define the problem in business terms.\n",
    "\n",
    "### Problem Statement\n",
    "Pokemon is the highest grossing media franchise of all time, leading ahead of other very popular franchises such as\n",
    "Disney's Marvel or Star Wars. The franchise has its roots in a creature-collection RPG video game released back in\n",
    "1996 for the GameBoy console. Since then, the franchise has expanded to include an animated series with 27 seasons as of\n",
    "2024, 23 movies, a trading card game spanning decades, a myriad of licensed toys and merchandise, a video-game series\n",
    "with 41 main games and countless spin-offs, and even a theme park in Japan.\n",
    "\n",
    "The franchise's main appeal is the fictitious creatures known as Pokemon, which are caught and trained by the people in\n",
    "the Pokemon world for battles. Each Pokemon has a unique style, ranging from cute to intimidating and everything in\n",
    "between, they are classified into one or two elemental types and has a set of unique characteristics that sets them\n",
    "apart.\n",
    "\n",
    "Telling what a Pokemon is from an image is a task trivial for humans, but as the series has grown from its original 151\n",
    "roaster of Pokemon to include the current 1025 species as of the latest additions in Pokemon Scarlet and Violet, it has\n",
    "become a daunting task for anyone to keep track of all the Pokemon species and their types. Additionally, as the series\n",
    "is sure to keep growing, more and more Pokemon will be added, making it even harder to keep track of them all.\n",
    "\n",
    "With this in mind, we have devised a project to help Pokemon fans identify whether a creature is a Pokemon and what type\n",
    "it is based on images, and to help identify even future Pokemon species. For this, an image classification model will be\n",
    "developed in order to be able to tell Pokemon apart from creatures in other similar competing franchises such as Digimon,\n",
    "as well as determine the elemental type of the Pokemon.\n",
    "\n",
    "Ideally, this model would be only the tip of the pyramid, additional features could be added to turn this into a more\n",
    "useful real application such as computer vision models to identify creatures from video footage for this model to classify,\n",
    "a recommendation system to suggest creatures with an advantage against the identified Pokemon, and much more.\n",
    "\n",
    "We hope this project will be a fun and interesting learning experience and that it builds a solid foundation.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "The following notebook will include every step, from gathering the data for our model to training and evaluating it.\n",
    "The main objective of this project is to develop a machine learning model capable of classifying images of fictitious\n",
    "creatures as Pokemon or not, and for those identified as Pokemon, to classify them into their respective elemental types.\n",
    "\n",
    "1. Determine if an image of a creature represents a Pokemon or not.\n",
    "2. Determine the elemental type of those creatures identified as Pokemon.\n",
    "3. Evaluate the performance of the model using appropriate metrics.\n",
    "\n",
    "#### Framing the problem\n",
    "##### Determine if an image of a creature represents a Pokemon or not.\n",
    "In the case of the first objective, the problem involves binary classification, where the model's output needs only\n",
    "to be a yes or no answer to the question \"Is this a Pokemon?\".\n",
    "\n",
    "Since we desire a model capable of identifying Pokemon, supervised learning is the most appropriate approach, as we have\n",
    "clearly labeled data to train the model on. An unsupervised approach would not be appropriate, as allowing the model to\n",
    "find patterns in the data on its own would be a waste of time at best and produce a model with inaccurate results at worst.\n",
    "\n",
    "To measure the performance of this model, there are a few metrics to take into account:\n",
    "  - The model's accuracy defined by how many guesses are actually correct.\n",
    "  - The model's precision defined as the fraction of the true predictions that are actually true.\n",
    "  $$ \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} $$\n",
    "  - The model's recall defined as the fraction of images correctly identified:\n",
    "  $$\\text{Recall}= \\frac {\\text{True Positives}}{\\text{True Positives + False Negatives}} $$\n",
    "  - F1 score is also an interesting metric, which is the mean of precision and recall. Since we're dealing with rates,\n",
    "  the harmonic mean is used instead of the arithmetic mean for most appropriate results:\n",
    "  $$\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "  - Area Under the Curve (AUC) is also a good metric to take into account, as it measures the area under the ROC curve,\n",
    "  which is a plot of the true positive rate against the false positive rate.\n",
    "  - Binary Cross-Entropy, which is the loss function used for  classification problems, is also a good metric to take\n",
    "  into account, as it measures the difference between the predicted probability distribution and the true distribution.\n",
    "\n",
    "##### Determine the Pokemon type\n",
    "In the case of the second objective, the problem involves a multi-label classification, where the model's output needs\n",
    "to be a list of types that the Pokemon belongs to. This is because some Pokemon species have more than one type, and the\n",
    "model needs to be able to identify all of them.\n",
    "\n",
    "Since we desire a model capable of identifying Pokemon types, supervised learning is the most appropriate approach, as we\n",
    "have clearly labeled data to train the model on. The same as before, we have already classified data to train our model\n",
    "with, and it helps us speed up the process to get higher accuracy.\n",
    "\n",
    "To measure the performance of this model, there are a few metrics to take into account, firstly, since this is also a\n",
    "classification problem, the same metrics as before can be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The following is the assignment description for the followning section, hidden from display\n",
    "### 2. Get the data\n",
    "1. Find and document where you can get the data from\n",
    "2. Get the data\n",
    "3. Check the size and type of data (time series, geographical etc)\n",
    "-->\n",
    "## 2. Get the data\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "To train the model for Pokémon classification, two datasets have been selected\n",
    "to cover the primary tasks: identifying Pokémon and distinguishing them from\n",
    "non-Pokémon images. These datasets provide a robust foundation for the project.\n",
    "\n",
    "- The PokeAPI dataset is an extensive dataset containing all sorts of information\n",
    "about the Pokemon universe, including information on all Pokemon species, images\n",
    "and other information such as moves, abilities, and locations. The dataset is\n",
    "available on GitHub [here](https://github.com/PokeAPI/pokeapi.git) and the\n",
    "corresponding submodules.\n",
    "\n",
    "- The Digimon dataset is a collection of images scraped from the\n",
    "[Wikimon.net](https://wikimon.net/Visual_List_of_Digimon) website and contains\n",
    "about 1000 images of Digimon species and variations, since this only serves as\n",
    "negative examples of Pokemon, we only need their images and none of the additional\n",
    "information about them.\n",
    "\n",
    "The web scrapper used to gather the Digimon data can be found on GitHub\n",
    "[here](https://github.com/lorenzo-stacchio/Digimon_Dataset), although there\n",
    "exists a Google Drive link with the data already gathered, which can be found\n",
    "[here](https://drive.google.com/drive/folders/1tmcdsoX67NvmAgtmGJgo6kb3N6SlJeLu?usp=share_link)\n",
    "\n",
    "In order for the rest of the notebook to properly be able to access the dataset, an additional\n",
    "path.env configuration file is needed, the following is a sample of said file contents:\n",
    "\n",
    "```properties\n",
    "pokeAPI-data=path/to/PokeAPI\n",
    "pokeAPI-sprites=path/to/PokeAPI-sprites\n",
    "digimon-images=path/to/Digimon\n",
    "\n",
    "dataset-dir=path/to/generated/dataset\n",
    "```\n",
    "\n",
    "Notably, `dataset-dir` is the directory where the generated dataset will be saved. In case that this is not the first\n",
    "time that you run this notebook, the dataset will be read from this directory instead of being generated again, this is\n",
    "a measure taken to save time and resources with subsequent runs of the notebook, during development where every session\n",
    "a new Jupyter server is started and the whole notebook is run again, this is a very useful feature to have."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:39.830301Z",
     "start_time": "2024-12-15T02:02:39.826270Z"
    }
   },
   "source": [
    "# This function reads the properties from the path.env file\n",
    "def read_properties(path):\n",
    "    props = {}\n",
    "    property_regex = re.compile(r'#{0}(.+)[:=]([^\\n\\r#]+)#?')\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            match = property_regex.match(line)\n",
    "            if match:\n",
    "                key = match.group(1).strip()\n",
    "                value = match.group(2).strip()\n",
    "                #print('Found property: {}={}'.format(key, value))\n",
    "                props[key] = value\n",
    "    return props"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:39.936206Z",
     "start_time": "2024-12-15T02:02:39.928731Z"
    }
   },
   "source": [
    "props = read_properties('paths.env')\n",
    "\n",
    "pokeAPI_data_repo = props['pokeAPI-data']\n",
    "pokeAPI_sprites_repo = props['pokeAPI-sprites']\n",
    "digimon_datasource = props['digimon-images']\n",
    "\n",
    "output_dir = props['dataset-dir']\n",
    "display(props)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pokeAPI-data': '/media/ieris19/development-drive/Development/Data/PokeAPI/server',\n",
       " 'pokeAPI-sprites': '/media/ieris19/development-drive/Development/Data/PokeAPI/sprites',\n",
       " 'digimon-images': '/media/ieris19/development-drive/Development/Data/Digimon',\n",
       " 'dataset-dir': '/media/ieris19/development-drive/Development/Data/MAL-project'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:39.996617Z",
     "start_time": "2024-12-15T02:02:39.992593Z"
    }
   },
   "source": [
    "# Using __ as a prefix to clarify that variables are not meant to be used later\n",
    "# PokeAPI csv data\n",
    "__pokeAPI_data_root = os.path.join(pokeAPI_data_repo, 'data/v2/csv')\n",
    "# PokeAPI sprites folder\n",
    "__pokeAPI_sprites_folder = os.path.join(pokeAPI_sprites_repo, 'sprites')\n",
    "# Digimon images folder\n",
    "__digimon_images_folder = os.path.join(digimon_datasource, 'images')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:40.241251Z",
     "start_time": "2024-12-15T02:02:40.120141Z"
    }
   },
   "source": [
    "print('Checking for data...')\n",
    "__pokeAPI_data_url = 'https://github.com/PokeAPI/pokeapi.git'\n",
    "__pokeAPI_sprites_url = 'https://github.com/PokeAPI/sprites.git'\n",
    "__digimon_data_url = 'https://drive.google.com/drive/folders/1tmcdsoX67NvmAgtmGJgo6kb3N6SlJeLu?usp=share_link'\n",
    "\n",
    "__missing_sources = []\n",
    "if not os.path.exists(__pokeAPI_data_root):\n",
    "    print('You must download the PokeAPI data first from the following git: {}'.format(__pokeAPI_data_url))\n",
    "    __missing_sources.append('PokeAPI data')\n",
    "else:\n",
    "    print('Retrieving PokeAPI data from: {}'.format(__pokeAPI_data_root))\n",
    "\n",
    "if not os.path.exists(__pokeAPI_sprites_folder):\n",
    "    print('You must download the PokeAPI sprites first from the following git: {}'.format(__pokeAPI_sprites_url))\n",
    "    __missing_sources.append('PokeAPI sprites')\n",
    "else:\n",
    "    print('Retrieving PokeAPI sprites from: {}'.format(__pokeAPI_sprites_folder))\n",
    "\n",
    "if not os.path.exists(__digimon_images_folder):\n",
    "    print('You must download the Digimon data first from the following Google Drive: {}'.format(__digimon_data_url))\n",
    "    __missing_sources.append('Digimon images')\n",
    "else:\n",
    "    print('Retrieving Digimon images from: {}'.format(__digimon_images_folder))\n",
    "\n",
    "if len(__missing_sources) > 1:\n",
    "    print('You must download the following sources: {}'.format(', '.join(__missing_sources)))\n",
    "    # TODO: If props['debug'] is False raise the error?, we can then control it from paths.env and rename it .env\n",
    "    # raise FileNotFoundError('Missing data sources')\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for data...\n",
      "Retrieving PokeAPI data from: /media/ieris19/development-drive/Development/Data/PokeAPI/server/data/v2/csv\n",
      "Retrieving PokeAPI sprites from: /media/ieris19/development-drive/Development/Data/PokeAPI/sprites/sprites\n",
      "Retrieving Digimon images from: /media/ieris19/development-drive/Development/Data/Digimon/images\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:40.249985Z",
     "start_time": "2024-12-15T02:02:40.246313Z"
    }
   },
   "source": [
    "# Verify the datasource hasn't generated earlier\n",
    "dataset_dir = os.path.join(output_dir, 'dataset')\n",
    "dataset_path = os.path.join(dataset_dir, 'pokemon.csv')\n",
    "generate_dataset = True\n",
    "if os.path.exists(dataset_path):\n",
    "    generate_dataset = False\n",
    "    print('The dataset has already been generated, please delete to regenerate')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has already been generated, please delete to regenerate\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The following is the assignment description for the followning section, hidden from display\n",
    "## 3. Explore the data\n",
    "1. Create a copy of the data for explorations (sampling it down to a manageable size if necessary)\n",
    "2. Create a Jupyter notebook to keep a record of your data exploration\n",
    "3. Study each feature and its characteristics:\n",
    "    * Name\n",
    "    * Type (categorical, int/float, bounded/unbounded, text, structured, etc)\n",
    "    * Percentage of missing values\n",
    "    * Check for outliers, rounding errors etc\n",
    "4. For supervised learning tasks, identify the target(s)\n",
    "5. Visualise the data\n",
    "6. Study the correlations between features\n",
    "7. Identify the promising transformations you may want to apply (e.g. convert skewed targets to normal via a log transformation)\n",
    "8. Document what you have learned\n",
    "-->\n",
    "#### Data exploration\n",
    "\n",
    "- PokeAPI Dataset:\n",
    "    - The dataset contains images for all Pokemon species, one for each game\n",
    "    they have been in among many alternate forms and color variations.\n",
    "    - The dataset also includes CSV files with information about Pokemon, where\n",
    "    to find them, their moves and abilities, etc...\n",
    "- Digimon Dataset\n",
    "    - The dataset contains images scraped from the Digimon wiki, this includes\n",
    "    roughly about a thousand images of Digimon species and variations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:47.601470Z",
     "start_time": "2024-12-15T02:02:40.301113Z"
    }
   },
   "source": [
    "# Exploring the available data\n",
    "def retrieve_relevant_files(path, extension) -> list:\n",
    "    available_files = []\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        if os.path.isdir(os.path.join(path, file)) and extension != '':\n",
    "            sub_files = retrieve_relevant_files(os.path.join(path, file), extension)\n",
    "            for sub_file in sub_files:\n",
    "                available_files.append(f'{file}/{sub_file}')\n",
    "        if os.path.isdir(os.path.join(path, file)) and extension == '':\n",
    "            available_files.append(file)\n",
    "            sub_dirs = retrieve_relevant_files(os.path.join(path, file), extension)\n",
    "            for sub_dir in sub_dirs:\n",
    "                available_files.append(f'{file}/{sub_dir}')\n",
    "        elif os.path.isfile(os.path.join(path, file)) and extension != '' and file.endswith(extension):\n",
    "            available_files.append(file)\n",
    "    return available_files\n",
    "\n",
    "def find_relevant_sources(path, extension, missing_key):\n",
    "    if missing_key not in __missing_sources:\n",
    "        print('-'*20)\n",
    "        print(f'{missing_key} files:')\n",
    "        for file in retrieve_relevant_files(path, extension):\n",
    "            print(file)\n",
    "    else:\n",
    "        print(f'{missing_key} files not found, skipping instrospection')\n",
    "\n",
    "find_relevant_sources(__pokeAPI_data_root, '.csv', 'PokeAPI data')\n",
    "find_relevant_sources(__pokeAPI_sprites_folder, '', 'PokeAPI sprites')\n",
    "find_relevant_sources(__digimon_images_folder, '', 'Digimon images')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "PokeAPI data files:\n",
      "abilities.csv\n",
      "ability_changelog.csv\n",
      "ability_changelog_prose.csv\n",
      "ability_flavor_text.csv\n",
      "ability_names.csv\n",
      "ability_prose.csv\n",
      "berries.csv\n",
      "berry_firmness.csv\n",
      "berry_firmness_names.csv\n",
      "berry_flavors.csv\n",
      "characteristics.csv\n",
      "characteristic_text.csv\n",
      "conquest_episodes.csv\n",
      "conquest_episode_names.csv\n",
      "conquest_episode_warriors.csv\n",
      "conquest_kingdom_names.csv\n",
      "conquest_max_links.csv\n",
      "conquest_move_data.csv\n",
      "conquest_move_displacements.csv\n",
      "conquest_move_displacement_prose.csv\n",
      "conquest_move_effects.csv\n",
      "conquest_move_effect_prose.csv\n",
      "conquest_move_ranges.csv\n",
      "conquest_move_range_prose.csv\n",
      "conquest_pokemon_abilities.csv\n",
      "conquest_pokemon_evolution.csv\n",
      "conquest_pokemon_moves.csv\n",
      "conquest_pokemon_stats.csv\n",
      "conquest_stats.csv\n",
      "conquest_transformation_pokemon.csv\n",
      "conquest_transformation_warriors.csv\n",
      "conquest_warriors.csv\n",
      "conquest_warrior_archetypes.csv\n",
      "conquest_warrior_names.csv\n",
      "conquest_warrior_ranks.csv\n",
      "conquest_warrior_rank_stat_map.csv\n",
      "conquest_warrior_skills.csv\n",
      "conquest_warrior_skill_names.csv\n",
      "conquest_warrior_specialties.csv\n",
      "conquest_warrior_stats.csv\n",
      "conquest_warrior_stat_names.csv\n",
      "conquest_warrior_transformation.csv\n",
      "contest_combos.csv\n",
      "contest_effect_prose.csv\n",
      "contest_types.csv\n",
      "contest_type_names.csv\n",
      "egg_groups.csv\n",
      "egg_group_prose.csv\n",
      "encounters.csv\n",
      "encounter_conditions.csv\n",
      "encounter_condition_prose.csv\n",
      "encounter_condition_values.csv\n",
      "encounter_condition_value_map.csv\n",
      "encounter_condition_value_prose.csv\n",
      "encounter_methods.csv\n",
      "encounter_method_prose.csv\n",
      "encounter_slots.csv\n",
      "evolution_chains.csv\n",
      "conquest_kingdoms.csv\n",
      "conquest_stat_names.csv\n",
      "contest_effects.csv\n",
      "evolution_triggers.csv\n",
      "item_fling_effect_prose.csv\n",
      "move_changelog.csv\n",
      "move_meta_category_prose.csv\n",
      "pokedex_version_groups.csv\n",
      "pokemon_habitat_names.csv\n",
      "region_names.csv\n",
      "evolution_trigger_prose.csv\n",
      "experience.csv\n",
      "genders.csv\n",
      "generations.csv\n",
      "generation_names.csv\n",
      "growth_rates.csv\n",
      "growth_rate_prose.csv\n",
      "items.csv\n",
      "item_categories.csv\n",
      "item_category_prose.csv\n",
      "item_flags.csv\n",
      "item_flag_map.csv\n",
      "item_flag_prose.csv\n",
      "item_flavor_summaries.csv\n",
      "item_flavor_text.csv\n",
      "item_fling_effects.csv\n",
      "item_game_indices.csv\n",
      "item_names.csv\n",
      "item_pockets.csv\n",
      "item_pocket_names.csv\n",
      "item_prose.csv\n",
      "languages.csv\n",
      "language_names.csv\n",
      "locations.csv\n",
      "location_areas.csv\n",
      "location_area_encounter_rates.csv\n",
      "location_area_prose.csv\n",
      "location_game_indices.csv\n",
      "location_names.csv\n",
      "machines.csv\n",
      "moves.csv\n",
      "move_battle_styles.csv\n",
      "move_battle_style_prose.csv\n",
      "move_damage_classes.csv\n",
      "move_damage_class_prose.csv\n",
      "move_effects.csv\n",
      "move_effect_changelog.csv\n",
      "move_effect_changelog_prose.csv\n",
      "move_effect_prose.csv\n",
      "move_flags.csv\n",
      "move_flag_map.csv\n",
      "move_flag_prose.csv\n",
      "move_flavor_summaries.csv\n",
      "move_flavor_text.csv\n",
      "move_meta.csv\n",
      "move_meta_ailments.csv\n",
      "move_meta_ailment_names.csv\n",
      "move_meta_categories.csv\n",
      "move_meta_stat_changes.csv\n",
      "move_names.csv\n",
      "move_targets.csv\n",
      "move_target_prose.csv\n",
      "natures.csv\n",
      "nature_battle_style_preferences.csv\n",
      "nature_names.csv\n",
      "nature_pokeathlon_stats.csv\n",
      "pal_park.csv\n",
      "pal_park_areas.csv\n",
      "pal_park_area_names.csv\n",
      "pokeathlon_stats.csv\n",
      "pokeathlon_stat_names.csv\n",
      "pokedexes.csv\n",
      "pokedex_prose.csv\n",
      "pokemon.csv\n",
      "pokemon_abilities.csv\n",
      "pokemon_abilities_past.csv\n",
      "pokemon_colors.csv\n",
      "pokemon_color_names.csv\n",
      "pokemon_dex_numbers.csv\n",
      "pokemon_egg_groups.csv\n",
      "pokemon_evolution.csv\n",
      "pokemon_forms.csv\n",
      "pokemon_form_generations.csv\n",
      "pokemon_form_names.csv\n",
      "pokemon_form_pokeathlon_stats.csv\n",
      "pokemon_form_types.csv\n",
      "pokemon_game_indices.csv\n",
      "pokemon_habitats.csv\n",
      "pokemon_items.csv\n",
      "pokemon_moves.csv\n",
      "pokemon_move_methods.csv\n",
      "pokemon_move_method_prose.csv\n",
      "pokemon_shapes.csv\n",
      "pokemon_shape_prose.csv\n",
      "pokemon_species.csv\n",
      "pokemon_species_flavor_summaries.csv\n",
      "pokemon_species_flavor_text.csv\n",
      "pokemon_species_names.csv\n",
      "pokemon_species_prose.csv\n",
      "pokemon_stats.csv\n",
      "pokemon_types.csv\n",
      "pokemon_types_past.csv\n",
      "regions.csv\n",
      "stats.csv\n",
      "stat_names.csv\n",
      "super_contest_combos.csv\n",
      "super_contest_effects.csv\n",
      "super_contest_effect_prose.csv\n",
      "translations/cs.csv\n",
      "types.csv\n",
      "type_efficacy.csv\n",
      "type_efficacy_past.csv\n",
      "type_game_indices.csv\n",
      "type_names.csv\n",
      "versions.csv\n",
      "version_groups.csv\n",
      "version_group_pokemon_move_methods.csv\n",
      "version_group_regions.csv\n",
      "version_names.csv\n",
      "--------------------\n",
      "PokeAPI sprites files:\n",
      "badges\n",
      "items\n",
      "items/berries\n",
      "items/dream-world\n",
      "items/gen3\n",
      "items/gen5\n",
      "items/underground\n",
      "pokemon\n",
      "pokemon/back\n",
      "pokemon/back/female\n",
      "pokemon/back/shiny\n",
      "pokemon/back/shiny/female\n",
      "pokemon/female\n",
      "pokemon/lowres\n",
      "pokemon/other\n",
      "pokemon/other/dream-world\n",
      "pokemon/other/dream-world/female\n",
      "pokemon/other/home\n",
      "pokemon/other/home/female\n",
      "pokemon/other/home/shiny\n",
      "pokemon/other/home/shiny/female\n",
      "pokemon/other/official-artwork\n",
      "pokemon/other/official-artwork/shiny\n",
      "pokemon/other/showdown\n",
      "pokemon/other/showdown/back\n",
      "pokemon/other/showdown/back/female\n",
      "pokemon/other/showdown/back/shiny\n",
      "pokemon/other/showdown/back/shiny/female\n",
      "pokemon/other/showdown/female\n",
      "pokemon/other/showdown/shiny\n",
      "pokemon/other/showdown/shiny/female\n",
      "pokemon/shiny\n",
      "pokemon/shiny/female\n",
      "pokemon/shiny/lowres\n",
      "pokemon/versions\n",
      "pokemon/versions/generation-i\n",
      "pokemon/versions/generation-i/red-blue\n",
      "pokemon/versions/generation-i/red-blue/back\n",
      "pokemon/versions/generation-i/red-blue/back/gray\n",
      "pokemon/versions/generation-i/red-blue/gray\n",
      "pokemon/versions/generation-i/red-blue/transparent\n",
      "pokemon/versions/generation-i/red-blue/transparent/back\n",
      "pokemon/versions/generation-i/yellow\n",
      "pokemon/versions/generation-i/yellow/back\n",
      "pokemon/versions/generation-i/yellow/back/gbc\n",
      "pokemon/versions/generation-i/yellow/back/gray\n",
      "pokemon/versions/generation-i/yellow/gbc\n",
      "pokemon/versions/generation-i/yellow/gray\n",
      "pokemon/versions/generation-i/yellow/transparent\n",
      "pokemon/versions/generation-i/yellow/transparent/back\n",
      "pokemon/versions/generation-ii\n",
      "pokemon/versions/generation-ii/crystal\n",
      "pokemon/versions/generation-ii/crystal/back\n",
      "pokemon/versions/generation-ii/crystal/back/shiny\n",
      "pokemon/versions/generation-ii/crystal/shiny\n",
      "pokemon/versions/generation-ii/crystal/transparent\n",
      "pokemon/versions/generation-ii/crystal/transparent/back\n",
      "pokemon/versions/generation-ii/crystal/transparent/back/shiny\n",
      "pokemon/versions/generation-ii/crystal/transparent/shiny\n",
      "pokemon/versions/generation-ii/gold\n",
      "pokemon/versions/generation-ii/gold/back\n",
      "pokemon/versions/generation-ii/gold/back/shiny\n",
      "pokemon/versions/generation-ii/gold/shiny\n",
      "pokemon/versions/generation-ii/gold/transparent\n",
      "pokemon/versions/generation-ii/silver\n",
      "pokemon/versions/generation-ii/silver/back\n",
      "pokemon/versions/generation-ii/silver/back/shiny\n",
      "pokemon/versions/generation-ii/silver/shiny\n",
      "pokemon/versions/generation-ii/silver/transparent\n",
      "pokemon/versions/generation-iii\n",
      "pokemon/versions/generation-iii/emerald\n",
      "pokemon/versions/generation-iii/emerald/shiny\n",
      "pokemon/versions/generation-iii/firered-leafgreen\n",
      "pokemon/versions/generation-iii/firered-leafgreen/back\n",
      "pokemon/versions/generation-iii/firered-leafgreen/back/shiny\n",
      "pokemon/versions/generation-iii/firered-leafgreen/shiny\n",
      "pokemon/versions/generation-iii/ruby-sapphire\n",
      "pokemon/versions/generation-iii/ruby-sapphire/back\n",
      "pokemon/versions/generation-iii/ruby-sapphire/back/shiny\n",
      "pokemon/versions/generation-iii/ruby-sapphire/shiny\n",
      "pokemon/versions/generation-iv\n",
      "pokemon/versions/generation-iv/diamond-pearl\n",
      "pokemon/versions/generation-iv/diamond-pearl/back\n",
      "pokemon/versions/generation-iv/diamond-pearl/back/female\n",
      "pokemon/versions/generation-iv/diamond-pearl/back/shiny\n",
      "pokemon/versions/generation-iv/diamond-pearl/back/shiny/female\n",
      "pokemon/versions/generation-iv/diamond-pearl/female\n",
      "pokemon/versions/generation-iv/diamond-pearl/shiny\n",
      "pokemon/versions/generation-iv/diamond-pearl/shiny/female\n",
      "pokemon/versions/generation-iv/heartgold-soulsilver\n",
      "pokemon/versions/generation-iv/heartgold-soulsilver/back\n",
      "pokemon/versions/generation-iv/heartgold-soulsilver/back/female\n",
      "pokemon/versions/generation-iv/heartgold-soulsilver/back/shiny\n",
      "pokemon/versions/generation-iv/heartgold-soulsilver/back/shiny/female\n",
      "pokemon/versions/generation-iv/heartgold-soulsilver/female\n",
      "pokemon/versions/generation-iv/heartgold-soulsilver/shiny\n",
      "pokemon/versions/generation-iv/heartgold-soulsilver/shiny/female\n",
      "pokemon/versions/generation-iv/platinum\n",
      "pokemon/versions/generation-iv/platinum/back\n",
      "pokemon/versions/generation-iv/platinum/back/female\n",
      "pokemon/versions/generation-iv/platinum/back/shiny\n",
      "pokemon/versions/generation-iv/platinum/back/shiny/female\n",
      "pokemon/versions/generation-iv/platinum/female\n",
      "pokemon/versions/generation-iv/platinum/shiny\n",
      "pokemon/versions/generation-iv/platinum/shiny/female\n",
      "pokemon/versions/generation-v\n",
      "pokemon/versions/generation-v/black-white\n",
      "pokemon/versions/generation-v/black-white/animated\n",
      "pokemon/versions/generation-v/black-white/animated/back\n",
      "pokemon/versions/generation-v/black-white/animated/back/female\n",
      "pokemon/versions/generation-v/black-white/animated/back/shiny\n",
      "pokemon/versions/generation-v/black-white/animated/back/shiny/female\n",
      "pokemon/versions/generation-v/black-white/animated/female\n",
      "pokemon/versions/generation-v/black-white/animated/shiny\n",
      "pokemon/versions/generation-v/black-white/animated/shiny/female\n",
      "pokemon/versions/generation-v/black-white/back\n",
      "pokemon/versions/generation-v/black-white/back/female\n",
      "pokemon/versions/generation-v/black-white/back/shiny\n",
      "pokemon/versions/generation-v/black-white/back/shiny/female\n",
      "pokemon/versions/generation-v/black-white/female\n",
      "pokemon/versions/generation-v/black-white/shiny\n",
      "pokemon/versions/generation-v/black-white/shiny/female\n",
      "pokemon/versions/generation-vi\n",
      "pokemon/versions/generation-vi/omegaruby-alphasapphire\n",
      "pokemon/versions/generation-vi/omegaruby-alphasapphire/female\n",
      "pokemon/versions/generation-vi/omegaruby-alphasapphire/shiny\n",
      "pokemon/versions/generation-vi/omegaruby-alphasapphire/shiny/female\n",
      "pokemon/versions/generation-vi/x-y\n",
      "pokemon/versions/generation-vi/x-y/female\n",
      "pokemon/versions/generation-vi/x-y/shiny\n",
      "pokemon/versions/generation-vi/x-y/shiny/female\n",
      "pokemon/versions/generation-vii\n",
      "pokemon/versions/generation-vii/icons\n",
      "pokemon/versions/generation-vii/icons/female\n",
      "pokemon/versions/generation-vii/ultra-sun-ultra-moon\n",
      "pokemon/versions/generation-vii/ultra-sun-ultra-moon/female\n",
      "pokemon/versions/generation-vii/ultra-sun-ultra-moon/shiny\n",
      "pokemon/versions/generation-vii/ultra-sun-ultra-moon/shiny/female\n",
      "pokemon/versions/generation-viii\n",
      "pokemon/versions/generation-viii/icons\n",
      "pokemon/versions/generation-viii/icons/female\n",
      "types\n",
      "types/generation-iii\n",
      "types/generation-iii/colosseum\n",
      "types/generation-iii/emerald\n",
      "types/generation-iii/firered-leafgreen\n",
      "types/generation-iii/ruby-saphire\n",
      "types/generation-iii/xd\n",
      "types/generation-iv\n",
      "types/generation-iv/diamond-pearl\n",
      "types/generation-iv/heartgold-soulsilver\n",
      "types/generation-iv/platinum\n",
      "types/generation-ix\n",
      "types/generation-ix/scarlet-violet\n",
      "types/generation-ix/scarlet-violet/Tera\n",
      "types/generation-v\n",
      "types/generation-v/black-2-white-2\n",
      "types/generation-v/black-white\n",
      "types/generation-vi\n",
      "types/generation-vi/omega-ruby-alpha-sapphire\n",
      "types/generation-vi/x-y\n",
      "types/generation-vii\n",
      "types/generation-vii/lets-go-pikachu-lets-go-eevee\n",
      "types/generation-vii/sun-moon\n",
      "types/generation-vii/ultra-sun-ultra-moon\n",
      "types/generation-viii\n",
      "types/generation-viii/brilliant-diamond-and-shining-pearl\n",
      "types/generation-viii/legends-arceus\n",
      "types/generation-viii/sword-shield\n",
      "--------------------\n",
      "Digimon images files:\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data:\n",
    "We can see there's an unending list of csv files in the PokeAPI dataset, most of these are not really relevant to us, as\n",
    "we are only interested in the Pokemon species and their types, as such, we will only be using the following files:\n",
    "- pokemon.csv\n",
    "    - This file contains the information of a Pokemon, such as its name, height, weight, and other information, we will\n",
    "    have to trim it but it's a start\n",
    "- types.csv\n",
    "    - This file contains an index of all types of Pokemon, such as fire, water, grass, etc... We will also have to trim\n",
    "    this as we're not interested in the damage class or generation of the type.\n",
    "- pokemon_types.csv\n",
    "    - This file contains the relationship between Pokemon and their types, we will use this to merge the Pokemon and\n",
    "    types so that we can have a single dataset with all our relevant information.\n",
    "\n",
    "#### The images\n",
    "As we can see, with the Digimon data it's pretty straightforward, we have a folder with all the images, since these are\n",
    "only negative examples of non-Pokemon creatures, we don't need to worry about any additional information about them, we\n",
    "can simply use all of them (despite many being alternate forms, color variations, etc...).\n",
    "\n",
    "As for the Pokemon images, we have many folders that can be immediately, folders such as badges and items for containing\n",
    "images that are irrelevant to our task at hand. As for the remaining, the types folder is promising but upon closer\n",
    "inspection, it contains images of the type labels in a badge, which isn't useful for us, the only folder that contains\n",
    "the images we need is the `pokemon` folder.\n",
    "\n",
    "In this folder, we have pictures of Pokemon as well as several subfolders, each containing even more pictures of the\n",
    "Pokemon. For briefness, we will refer to the [dataset-readme](https://github.com/PokeAPI/sprites/blob/master/README.md)\n",
    "in order to obtain more information about the images. We can see that the images are named after the Pokemon's ID, and\n",
    "that they're divided depending on the source game. Most of these contain pixel-art images of the Pokemon, but there is\n",
    "some promising folders that align with the art-style of our negative set of Digimon images, which makes it so that the\n",
    "model will have to rely on the features of the Pokemon to classify them, rather than the art-style.\n",
    "\n",
    "The most promising folders are the `other/official-artwork` which contain artist renditions of every Pokemon and the\n",
    "`other/dream-world` which contain images of the Pokemon in a more cartoony style from the browser game Pokemon Dream World.\n",
    "However, our decision is made easy knowing that the `official-artwork` folder contains images of all Pokemon to date while\n",
    "the `dream-world` folder only contains images of the first 649 Pokemon, which is a subset of the `official-artwork` folder.\n",
    "\n",
    "This means that we will only be using the `official-artwork` folder for our model, as it contains all the images we need.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:47.616674Z",
     "start_time": "2024-12-15T02:02:47.612241Z"
    }
   },
   "source": [
    "def import_data():\n",
    "    original_pokemon = pd.read_csv(os.path.join(__pokeAPI_data_root, 'pokemon.csv'))\n",
    "    original_types = pd.read_csv(os.path.join(__pokeAPI_data_root, 'types.csv'))\n",
    "    original_pokemon_types = pd.read_csv(os.path.join(__pokeAPI_data_root, 'pokemon_types.csv'))\n",
    "    return original_pokemon, original_types, original_pokemon_types"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trimming the data\n",
    "\n",
    "The PokeAPI dataset is quite extensive, it contains all Pokemon (currently 1025)\n",
    "and also many alternative form information. This information is assigned an ID\n",
    "that is greater than 10000 so that it does not interfere with the original Pokemon\n",
    "IDs, as such, we must filter out all IDs greater than 10000.\n",
    "\n",
    "Additionally, we will remove a lot of the columns that are not relevant to our\n",
    "current analysis, such as the foreign keys pointing to relationships outside our\n",
    "scope and some of the data irrelevant to us such as height and weight.\n",
    "\n",
    "There's also some types that are not relevant to our analysis, because they are\n",
    "only used for alternate forms of Pokemon which we have removed already, such as\n",
    "the \"shadow\" type. We will remove these types from the dataset, thankfully, the\n",
    "same as Pokemon IDs apply, these types have IDs greater than 10000.\n",
    "\n",
    "We will also rename some of the columns to make them more readable and to avoid\n",
    "confusion later on.\n",
    "\n",
    "The following are the functions that will generate a new trimmed dataset for us\n",
    "to use in our model. However, as mentioned earlier, before calling these functions\n",
    "we will check if the dataset has already been generated, if it has, we will read\n",
    "it instead to save time and resources."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:47.660225Z",
     "start_time": "2024-12-15T02:02:47.656496Z"
    }
   },
   "source": [
    "def clean_source_datasets(original_pokemon, original_types, original_pokemon_types):\n",
    "    id_cutoff = 10000\n",
    "\n",
    "    pokemon_types = original_pokemon_types[original_pokemon_types['pokemon_id'] < id_cutoff]\n",
    "    pokemon = original_pokemon[original_pokemon['id'] < id_cutoff]\n",
    "    types = original_types[original_types['id'] < id_cutoff]\n",
    "    types = types.drop(columns=['damage_class_id', 'generation_id'])\n",
    "    pokemon = pokemon.drop(columns=['species_id', 'height', 'weight', 'base_experience', 'order', 'is_default'])\n",
    "    pokemon = pokemon.rename(columns={'identifier': 'name'})\n",
    "    types = types.rename(columns={'identifier': 'type_label'})\n",
    "    return pokemon, types, pokemon_types"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:47.717314Z",
     "start_time": "2024-12-15T02:02:47.712490Z"
    }
   },
   "source": [
    "def merge_dataset(pokemon, types, pokemon_types):\n",
    "    merged = pokemon_types.merge(types, left_on='type_id', right_on='id').drop(columns=['id'])\n",
    "    merged = merged.rename(columns={'identifier': 'type', 'slot': 'type_slot'})\n",
    "    merged = pokemon.merge(merged, left_on='id', right_on='pokemon_id').drop(columns=['id'])\n",
    "    dataset_target = os.path.join(dataset_dir, 'pokemon.csv')\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "    merged.to_csv(dataset_target, index=False)\n",
    "    print('Pokemon data saved to {} for saving'.format(dataset_target))\n",
    "    return merged"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:47.881601Z",
     "start_time": "2024-12-15T02:02:47.757202Z"
    }
   },
   "source": [
    "# Generate or read the dataset based on the previous checks\n",
    "if generate_dataset:\n",
    "    pokemon_merged = merge_dataset(*clean_source_datasets(*import_data()))\n",
    "else:\n",
    "    print('Reading dataset from {}'.format(dataset_path))\n",
    "    pokemon_merged = pd.read_csv(dataset_path)\n",
    "pokemon_merged"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset from /media/ieris19/development-drive/Development/Data/MAL-project/dataset/pokemon.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "            name  pokemon_id  type_id  type_slot type_label\n",
       "0      bulbasaur           1       12          1      grass\n",
       "1      bulbasaur           1        4          2     poison\n",
       "2        ivysaur           2       12          1      grass\n",
       "3        ivysaur           2        4          2     poison\n",
       "4       venusaur           3       12          1      grass\n",
       "...          ...         ...      ...        ...        ...\n",
       "1546  iron-crown        1023        9          1      steel\n",
       "1547  iron-crown        1023       14          2    psychic\n",
       "1548   terapagos        1024        1          1     normal\n",
       "1549   pecharunt        1025        4          1     poison\n",
       "1550   pecharunt        1025        8          2      ghost\n",
       "\n",
       "[1551 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pokemon_id</th>\n",
       "      <th>type_id</th>\n",
       "      <th>type_slot</th>\n",
       "      <th>type_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bulbasaur</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bulbasaur</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ivysaur</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ivysaur</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>venusaur</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>iron-crown</td>\n",
       "      <td>1023</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>iron-crown</td>\n",
       "      <td>1023</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>psychic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>terapagos</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>pecharunt</td>\n",
       "      <td>1025</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>pecharunt</td>\n",
       "      <td>1025</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ghost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1551 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:47.937242Z",
     "start_time": "2024-12-15T02:02:47.932569Z"
    }
   },
   "source": [
    "# Check if images have already been extracted:\n",
    "images_dir = os.path.join(output_dir, 'images')\n",
    "extract_images = {}\n",
    "pokemon_images_dir = os.path.join(images_dir, 'pokemon')\n",
    "digimon_images_dir = os.path.join(images_dir, 'digimon')\n",
    "extract_images['pokemon'] = not os.path.exists(pokemon_images_dir)\n",
    "extract_images['digimon'] = not os.path.exists(digimon_images_dir)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:47.990198Z",
     "start_time": "2024-12-15T02:02:47.985037Z"
    }
   },
   "source": [
    "def extract_pokemon_images():\n",
    "    source_images = os.path.join(__pokeAPI_sprites_folder, 'pokemon/other/official-artwork')\n",
    "    poke_image_files = os.listdir(source_images)\n",
    "    print('Preparing {} Pokemon images...'.format(len(poke_image_files)))\n",
    "    os.makedirs(pokemon_images_dir)\n",
    "    # Like the data, images with ID higher than 10000 are not relevant\n",
    "    valid_image_pattern = re.compile(r'^[0-9]{1,4}\\.png')\n",
    "    for file in poke_image_files:\n",
    "        if valid_image_pattern.match(file):\n",
    "            source = os.path.join(source_images, file)\n",
    "            target = os.path.join(pokemon_images_dir, file)\n",
    "            shutil.copy(source, target)\n",
    "            print('Copied {} to {}'.format(file, target))"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:48.287660Z",
     "start_time": "2024-12-15T02:02:48.282883Z"
    }
   },
   "source": [
    "def extract_digimon_images():\n",
    "    digimon_image_files = os.listdir(__digimon_images_folder)\n",
    "    print('Preparing {} Digimon images...'.format(len(digimon_image_files)))\n",
    "    os.makedirs(digimon_images_dir)\n",
    "    for file in digimon_image_files:\n",
    "        source = os.path.join(__digimon_images_folder, file)\n",
    "        target = os.path.join(digimon_images_dir, file)\n",
    "        shutil.copy(source, target)\n",
    "        print('Copied file\\n\\tfrom: {}\\n\\tto: {}'.format(file, target))"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:48.709297Z",
     "start_time": "2024-12-15T02:02:48.703963Z"
    }
   },
   "source": [
    "if extract_images['pokemon']:\n",
    "    extract_pokemon_images()\n",
    "else:\n",
    "    print('Pokemon images already extracted')\n",
    "if extract_images['digimon']:\n",
    "    extract_digimon_images()\n",
    "else:\n",
    "    print('Digimon images already extracted')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pokemon images already extracted\n",
      "Digimon images already extracted\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//] # TODO: Add some visualization here to show some images and maybe some graphs\n",
    "## Dataset Visualization\n",
    "\n",
    "The following graphs give us a small insight into the relationship between Pokemon\n",
    "and types, as well as the distribution of types across all existing Pokemon.\n",
    "\n",
    "The first graph shows the count of Pokemon per type, please note, that Pokemon with\n",
    "two types will be counted once in each type so that the sum of the counts will be\n",
    "greater than the total number of Pokemon. This isn't a problem because we're\n",
    "looking at the distribution of types, not the distribution of Pokemon."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:49.538731Z",
     "start_time": "2024-12-15T02:02:48.945473Z"
    }
   },
   "source": [
    "sns.displot(data=pokemon_merged['type_label'])\n",
    "\n",
    "plt.title('Pokemon Types Distribution')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Amount of Pokemon')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "pokemon_merged['type_label'].value_counts().describe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count     18.000000\n",
       "mean      86.166667\n",
       "std       27.931113\n",
       "min       48.000000\n",
       "25%       69.000000\n",
       "50%       74.500000\n",
       "75%       99.500000\n",
       "max      154.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:49.608456Z",
     "start_time": "2024-12-15T02:02:49.601120Z"
    }
   },
   "source": [
    "# Let's look at how many Pokemon have more than one type\n",
    "all_pokemon = pokemon_merged[pokemon_merged['type_slot'] == 1]\n",
    "multi_type_pokemon = pokemon_merged[pokemon_merged['type_slot'] > 1]\n",
    "\n",
    "all_pokemon_count = all_pokemon.__len__()\n",
    "multi_type_pokemon_count = multi_type_pokemon.__len__()\n",
    "\n",
    "multi_type_percent = (multi_type_pokemon_count / all_pokemon_count) * 100\n",
    "\n",
    "print(f'There are {multi_type_pokemon.__len__()} Pokemon with more than one type')\n",
    "print(f'This is {multi_type_percent:.2f}% of all Pokemon')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 526 Pokemon with more than one type\n",
      "This is 51.32% of all Pokemon\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- [//] # TODO: Do we really account for this? -->\n",
    "The previous calculation shows that 51.32% of all Pokemon have more than one type,\n",
    "this is a more than significant amount of Pokemon that have more than one type,\n",
    "as such, we cannot limit ourselves to guessing only one type per Pokemon, we must\n",
    "be able to guess multiple types per Pokemon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The following is the assignment description for the followning section, hidden from display\n",
    "## 4. Prepare the data\n",
    "Notes:\n",
    "* Work on copies of the data (keep the original dataset intact).\n",
    "* Write functions for all data transformations you apply, for three reasons:\n",
    "    * So you can easily prepare the data the next time you run your code\n",
    "    * So you can apply these transformations in future projects\n",
    "    * To clean and prepare the test set\n",
    "\n",
    "1. Data cleaning:\n",
    "    * Fix or remove outliers (or keep them)\n",
    "    * Fill in missing values (e.g. with zero, mean, median, regression ...) or drop their rows (or columns)\n",
    "2. Feature selection (optional):\n",
    "    * Drop the features that provide no useful information for the task (e.g. a customer ID is usually useless for modelling).\n",
    "3. Feature engineering, where appropriate:\n",
    "    * Discretize continuous features\n",
    "    * Use one-hot encoding if/when relevant\n",
    "    * Add promising transformations of features (e.g. $\\log(x)$, $\\sqrt{x}$, $x^2$, etc)\n",
    "    * Aggregate features into promising new features\n",
    "4. Feature scaling: standardise or normalise features\n",
    "-->\n",
    "## 4. Preparation\n",
    "\n",
    "Our extracted .csv data has already been trimmed to just the necessary columns, so\n",
    "not much will be needed there, since we are dealing with a full dataset there's\n",
    "no missing values to fill, nor does categorical data need to be adjusted for\n",
    "outliers.\n",
    "\n",
    "Thankfully, Pokemon have assinged IDs that are unique and match the images already,\n",
    "as well as the beneficial quirk of starting with normalized data, it gives us already\n",
    "a numerical ID for all types that we very intentionally left in the dataset, thus,\n",
    "we have both labels, and numerical values for all the variables we're interested\n",
    "in our tasks.\n",
    "\n",
    "As such, we can safely move on to processing the images.\n",
    "\n",
    "### Image Processing\n",
    "From our research, images will need two main transformations:\n",
    "- The must be resized to a common smaller size, this means both the Pokemon and\n",
    "Digimon images will have the same size, equalizing our inputs, but also reducing\n",
    "the computational cost of processing the images.\n",
    "- The images can be converted to a uniform image format (PNG in our case) for\n",
    "consistency.\n",
    "\n",
    "Additional optimizations have been made to the images, such as adding a white\n",
    "background to the PNG images with transparency, this is to ensure that the\n",
    "model can't fixate itself on the transparency of the images, as well as to\n",
    "ensure that the images are all as uniform as possible, removing as much possible\n",
    "bias as possible from the images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:49.738607Z",
     "start_time": "2024-12-15T02:02:49.680734Z"
    }
   },
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "final_img_dir = os.path.join(output_dir, 'pre-processed')\n",
    "# The following tuple defines the final resolution of the images\n",
    "new_size = (128, 128)\n",
    "\n",
    "pkm_output = os.path.join(final_img_dir, 'pokemon')\n",
    "dmn_output = os.path.join(final_img_dir, 'digimon')\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(final_img_dir, exist_ok=True)\n",
    "os.makedirs(pkm_output, exist_ok=True)\n",
    "os.makedirs(dmn_output, exist_ok=True)\n",
    "\n",
    "def resize_images(input_folder, output_folder, size):\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        file_extension = os.path.splitext(filename)[1].lower()\n",
    "        try:\n",
    "            with Image.open(input_path) as img:\n",
    "                img.thumbnail(size)  # Resizes while maintaining aspect ratio\n",
    "                if file_extension in ['.jpg', '.jpeg']:\n",
    "                    img = img.convert('RGB')  # Ensure JPEG images are in RGB mode\n",
    "                    output_path = os.path.splitext(output_path)[0] + '.png'  # Change extension to .png\n",
    "                    img.save(output_path, 'PNG', icc_profile=None)\n",
    "                else:\n",
    "                    img = img.convert('RGBA')\n",
    "                    # Create a white background\n",
    "                    background = Image.new('RGBA', img.size, (255, 255, 255, 255))\n",
    "                    # Composite the image with the white background\n",
    "                    img = Image.alpha_composite(background, img).convert('RGB')\n",
    "                    img.save(output_path, 'PNG', icc_profile=None)\n",
    "                print(f\"Resized and saved: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {filename}: {e}\")\n",
    "\n",
    "# Check if output_folder is empty\n",
    "if not os.listdir(pkm_output) and not os.listdir(dmn_output):\n",
    "    print(f\"The output folder {final_img_dir} is empty. Resizing images.\")\n",
    "    resize_images(pokemon_images_dir, pkm_output, new_size)\n",
    "    resize_images(digimon_images_dir, dmn_output, new_size)\n",
    "else:\n",
    "    print(f\"The output folder {final_img_dir} is not empty. Skipping resizing.\")\n",
    "# resize_images(pokemon_images_dir, os.path.join(final_img_dir, 'pokemon'), new_size)\n",
    "# resize_images(digimon_images_dir, os.path.join(final_img_dir, 'digimon'), new_size)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output folder /media/ieris19/development-drive/Development/Data/MAL-project/pre-processed is not empty. Skipping resizing.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <!-- The following is the assignment description for the followning section, hidden from display\n",
    "## 5. Short-list promising models\n",
    "We expect you to do some additional research and train at **least one model per team member**.\n",
    "\n",
    "1. Train mainly quick and dirty models from different categories (e.g. linear, SVM, Random Forests etc) using default parameters\n",
    "2. Measure and compare their performance\n",
    "3. Analyse the most significant variables for each algorithm\n",
    "4. Analyse the types of errors the models make\n",
    "5. Have a quick round of feature selection and engineering if necessary\n",
    "6. Have one or two more quick iterations of the five previous steps\n",
    "7. Short-list the top three to five most promising models, preferring models that make different types of errors\n",
    "-->\n",
    "<!-- [//] # TODO: This is where we train two models !!! One for each task OR remove one of the tasks? -->\n",
    "## 5. Promising models test\n",
    "\n",
    "Image classification is a well-known problem in the field of machine learning,\n",
    "and probably one of the most widely used applications of machine learning. As\n",
    "such, we can deduce that the most commonly used models for image classification\n",
    "will be the most promising for our task. Support Vector Machines and Neural Networks\n",
    "are the most commonly used models for image classification, however, SVM\n",
    "specifically is not well-suited as it is best suited for problems of linear\n",
    "classification.\n",
    "\n",
    "Neural Networks are a much more versatile model, and they can be used for a wide\n",
    "variety of tasks, including image classification. Convolutional Neural Networks\n",
    "are a type of neural network that is almost synonymous with image classification,\n",
    "being the most common model used for these kind of tasks, however, for the purpose\n",
    "of verifying this idea, we will test a simple Feedforward Neural Network and a\n",
    "Convolutional Neural Network to see which one performs better.\n",
    "\n",
    "## Final Dataset Preparation\n",
    "Before we actually, begin working with the data, we need to prepare it in such a\n",
    "way that the models can easily process them. Keras, a utility within TensorFlow\n",
    "allows us to quickly preprocess and prepare the data for Neural Network training.\n",
    "\n",
    "We will ensure images are the right size (although they've already been resized)\n",
    "and define the other parameters so that we can train several neural networks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:49.970428Z",
     "start_time": "2024-12-15T02:02:49.967301Z"
    }
   },
   "source": [
    "# Dataset properties\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 2000"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:50.163148Z",
     "start_time": "2024-12-15T02:02:50.157280Z"
    }
   },
   "source": [
    "# Declare a function to load data\n",
    "def data_loader(path, subset):\n",
    "    if subset:\n",
    "        return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            path,\n",
    "            interpolation='area',\n",
    "            validation_split=0.2,\n",
    "            subset=subset,\n",
    "            shuffle=True,\n",
    "            seed=123,\n",
    "            image_size=(IMG_SIZE, IMG_SIZE),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "    else:\n",
    "        return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            path,\n",
    "            interpolation='area',\n",
    "            shuffle=True,\n",
    "            seed=123,\n",
    "            image_size=(IMG_SIZE, IMG_SIZE),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:53.015657Z",
     "start_time": "2024-12-15T02:02:50.247734Z"
    }
   },
   "source": [
    "# Split the dataset into appropriate portions\n",
    "train_ds = data_loader(final_img_dir, 'training')\n",
    "\n",
    "val_ds = data_loader(final_img_dir, 'validation')\n",
    "\n",
    "test_ds = data_loader(final_img_dir, None)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2152 files belonging to 2 classes.\n",
      "Using 1722 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734228170.549991  147948 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734228171.091709  147948 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734228171.091944  147948 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734228171.093502  147948 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734228171.093740  147948 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734228171.093898  147948 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734228171.255476  147948 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734228171.255810  147948 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734228171.255977  147948 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-15 03:02:51.256094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3486 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2152 files belonging to 2 classes.\n",
      "Using 430 files for validation.\n",
      "Found 2152 files belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### FNN\n",
    "#### Load the data\n",
    "We will mainly use Tensorflow and Keras to build our models, as they are the most\n",
    "widely used and de-facto standard for building neural networks. We will start by\n",
    "loading the data and then building the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:02:53.989477Z",
     "start_time": "2024-12-15T02:02:53.035645Z"
    }
   },
   "source": [
    "# Build Feedforward neural network\n",
    "fnn_model = models.Sequential([\n",
    "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)), # 3 channels for RGB\n",
    "    layers.Rescaling(1./255), # normalize pixel values to [0, 1]\n",
    "    layers.Flatten(), # flatten the 128x128x3 input images\n",
    "    layers.Dense(512, activation='relu'), # fully connected layer with 512 units and ReLU activation\n",
    "    layers.Dropout(0.25), # dropout layer to prevent overfitting, drops 25% of the units\n",
    "    layers.Dense(256, activation='relu'), # fully connected layer with 256 units and ReLU activation\n",
    "    layers.Dropout(0.125), # dropout layer to prevent overfitting, drops 12.5% of the units\n",
    "    layers.Dense(1, activation='sigmoid') # output layer with 1 unit and sigmoid activation for binary classification\n",
    "], name='FNN_Test')\n",
    "\n",
    "# compile model\n",
    "fnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'Precision', 'Recall', 'AUC'],\n",
    "    jit_compile=True\n",
    ")\n",
    "\n",
    "# inspect model\n",
    "fnn_model.summary()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"FNN_Test\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"FNN_Test\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling (\u001B[38;5;33mRescaling\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m3\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001B[38;5;33mFlatten\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m49152\u001B[0m)          │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)            │    \u001B[38;5;34m25,166,336\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001B[38;5;33mDropout\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │       \u001B[38;5;34m131,328\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │           \u001B[38;5;34m257\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49152</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">25,166,336</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m25,297,921\u001B[0m (96.50 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,297,921</span> (96.50 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m25,297,921\u001B[0m (96.50 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,297,921</span> (96.50 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:03:40.037635Z",
     "start_time": "2024-12-15T02:02:54.023893Z"
    }
   },
   "source": [
    "# stop when val_accuracy doesnt improve\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training = fnn_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=25,\n",
    "    steps_per_epoch=2000 // BATCH_SIZE,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# evaluate on test set\n",
    "test_loss, test_acc, test_precision, test_recall, test_auc = fnn_model.evaluate(test_ds)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test precision: {test_precision:.4f}\")\n",
    "print(f\"Test recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1: {statistics.harmonic_mean([test_precision, test_recall]):.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "# print(f\"Test loss: {test_loss:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734228177.265431  148058 service.cc:146] XLA service 0x7f00a000c3a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1734228177.265457  148058 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce GTX 1050 Ti, Compute Capability 6.1\n",
      "2024-12-15 03:02:57.344467: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-15 03:02:57.570199: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 90101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m 3/62\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m1s\u001B[0m 18ms/step - AUC: 0.4693 - Precision: 0.5580 - Recall: 0.3085 - accuracy: 0.4655 - loss: 32.4204: 32.4204"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734228180.145584  148058 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m54/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m3s\u001B[0m 477ms/step - AUC: 0.4991 - Precision: 0.4930 - Recall: 0.4734 - accuracy: 0.5021 - loss: 16.0886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:03:25.428269: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-12-15 03:03:25.428306: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "/home/ieris19/.conda/envs/MachineLearning/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n",
      "2024-12-15 03:03:31.539449: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 100663296 exceeds 10% of free system memory.\n",
      "2024-12-15 03:03:31.777106: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 100663296 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 523ms/step - AUC: 0.4978 - Precision: 0.4905 - Recall: 0.4743 - accuracy: 0.5014 - loss: 15.3158 - val_AUC: 0.7123 - val_Precision: 0.6750 - val_Recall: 0.1337 - val_accuracy: 0.5628 - val_loss: 0.6667\n",
      "Epoch 2/25\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 14ms/step - AUC: 0.5260 - Precision: 0.4950 - Recall: 0.5006 - accuracy: 0.5134 - loss: 1.8245 - val_AUC: 0.6978 - val_Precision: 0.5500 - val_Recall: 0.0545 - val_accuracy: 0.5349 - val_loss: 0.6731\n",
      "Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:03:32.879122: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m50/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - AUC: 0.5604 - Precision: 0.5459 - Recall: 0.5447 - accuracy: 0.5506 - loss: 0.7055"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:03:33.872155: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 100663296 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 18ms/step - AUC: 0.5626 - Precision: 0.5457 - Recall: 0.5425 - accuracy: 0.5517 - loss: 0.7053 - val_AUC: 0.6603 - val_Precision: 0.6730 - val_Recall: 0.5297 - val_accuracy: 0.6581 - val_loss: 0.6798\n",
      "Epoch 4/25\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 13ms/step - AUC: 0.4934 - Precision: 0.5508 - Recall: 0.0447 - accuracy: 0.5329 - loss: 0.6918 - val_AUC: 0.5085 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.5302 - val_loss: 0.6904\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:03:34.866924: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2024-12-15 03:03:34.866955: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 12005889478531810639\n",
      "2024-12-15 03:03:34.866969: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17281436893529773292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 13ms/step - AUC: 0.5044 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.5244 - loss: 0.6920 - val_AUC: 0.5000 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.5302 - val_loss: 0.6917\n",
      "Epoch 6/25\n",
      "\u001B[1m 1/62\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m1s\u001B[0m 26ms/step - AUC: 0.5625 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.4844 - loss: 0.6945"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:03:35.692275: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 12005889478531810639\n",
      "2024-12-15 03:03:35.694373: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17281436893529773292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 13ms/step - AUC: 0.5362 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.5228 - loss: 0.6923 - val_AUC: 0.4978 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.5302 - val_loss: 0.6916\n",
      "Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:03:36.521048: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 12005889478531810639\n",
      "2024-12-15 03:03:36.521089: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17281436893529773292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 13ms/step - AUC: 0.4889 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.5246 - loss: 0.6920 - val_AUC: 0.4978 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.5302 - val_loss: 0.6916\n",
      "Epoch 8/25\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 13ms/step - AUC: 0.4967 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.5285 - loss: 0.6918 - val_AUC: 0.5000 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.5302 - val_loss: 0.6915\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:03:38.191648: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2024-12-15 03:03:38.191685: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 12005889478531810639\n",
      "2024-12-15 03:03:38.191745: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17281436893529773292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m20/68\u001B[0m \u001B[32m━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - AUC: 0.7071 - Precision: 0.7480 - Recall: 0.5551 - accuracy: 0.6912 - loss: 0.6724"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:03:38.588382: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 100663296 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m68/68\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 20ms/step - AUC: 0.6943 - Precision: 0.7126 - Recall: 0.5699 - accuracy: 0.6813 - loss: 0.6743\n",
      "\n",
      "Test accuracy: 0.6747\n",
      "Test precision: 0.6979\n",
      "Test recall: 0.5590\n",
      "Test F1: 0.6208\n",
      "Test AUC: 0.6842\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "A simple FNN is a good enough start so we can work through adding Convolution\n",
    "into our Neural Network to see if we can get better results, with this, the new\n",
    "Neural Network will hopefully perform better, since images have enormous\n",
    "dimensionality, convolution can help reduce it and thus allow the system to make\n",
    "more accurate predictions for less computational cost"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:04:54.651283Z",
     "start_time": "2024-12-15T02:04:54.568692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the CNN model\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    layers.Rescaling(1./255),  # Normalize pixel values to [0, 1]\n",
    "\n",
    "    layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.BatchNormalization(),# Flatten the image input\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "\n",
    "    layers.Dense(256, activation='relu'),  # Fully connected layer\n",
    "    layers.Dropout(0.25),  # Dropout to prevent overfitting\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "], name='CNN_Test')\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=0.0001),  # Optimizer with default learning rate\n",
    "    loss='binary_crossentropy',  # Loss function for binary classification\n",
    "    metrics=['accuracy', 'Precision', 'Recall', 'AUC'],  # Evaluation metrics\n",
    "    jit_compile=True  # Enable JIT compilation for speedup\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "cnn_model.summary()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"CNN_Test\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_Test\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling_2 (\u001B[38;5;33mRescaling\u001B[0m)         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m3\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │         \u001B[38;5;34m3,584\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │           \u001B[38;5;34m512\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │        \u001B[38;5;34m33,024\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │           \u001B[38;5;34m257\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m37,377\u001B[0m (146.00 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,377</span> (146.00 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m37,121\u001B[0m (145.00 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,121</span> (145.00 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m256\u001B[0m (1.00 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> (1.00 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:05:38.362534Z",
     "start_time": "2024-12-15T02:04:54.702080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "cnn_model_history = cnn_model.fit(\n",
    "    train_ds,  # Replace with your training dataset\n",
    "    validation_data=val_ds,  # Replace with your validation dataset\n",
    "    epochs=10,  # Number of epochs\n",
    "    batch_size=32  # Batch size\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "results = cnn_model.evaluate(test_ds)\n",
    "print(f\"Test Results - Loss: {results[0]}, Accuracy: {results[1]}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m54/54\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 185ms/step - AUC: 0.7191 - Precision: 0.6495 - Recall: 0.6339 - accuracy: 0.6715 - loss: 0.6625 - val_AUC: 0.7992 - val_Precision: 0.4698 - val_Recall: 1.0000 - val_accuracy: 0.4698 - val_loss: 0.6922\n",
      "Epoch 2/10\n",
      "\u001B[1m54/54\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 50ms/step - AUC: 0.8656 - Precision: 0.7686 - Recall: 0.8315 - accuracy: 0.7990 - loss: 0.5551 - val_AUC: 0.8823 - val_Precision: 0.4915 - val_Recall: 1.0000 - val_accuracy: 0.5140 - val_loss: 0.6779\n",
      "Epoch 3/10\n",
      "\u001B[1m54/54\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 49ms/step - AUC: 0.9070 - Precision: 0.7879 - Recall: 0.8722 - accuracy: 0.8271 - loss: 0.4811 - val_AUC: 0.9315 - val_Precision: 0.8318 - val_Recall: 0.9059 - val_accuracy: 0.8698 - val_loss: 0.6517\n",
      "Epoch 4/10\n",
      "\u001B[1m54/54\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 50ms/step - AUC: 0.9334 - Precision: 0.8194 - Recall: 0.9037 - accuracy: 0.8587 - loss: 0.4140 - val_AUC: 0.9560 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.5302 - val_loss: 0.6286\n",
      "Epoch 5/10\n",
      "\u001B[1m54/54\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 50ms/step - AUC: 0.9465 - Precision: 0.8479 - Recall: 0.9327 - accuracy: 0.8873 - loss: 0.3655 - val_AUC: 0.9604 - val_Precision: 0.9268 - val_Recall: 0.1881 - val_accuracy: 0.6116 - val_loss: 0.5809\n",
      "Epoch 6/10\n",
      "\u001B[1m54/54\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 50ms/step - AUC: 0.9497 - Precision: 0.8445 - Recall: 0.9284 - accuracy: 0.8841 - loss: 0.3343 - val_AUC: 0.9608 - val_Precision: 1.0000 - val_Recall: 0.0149 - val_accuracy: 0.5372 - val_loss: 0.5812\n",
      "Epoch 7/10\n",
      "\u001B[1m54/54\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 50ms/step - AUC: 0.9598 - Precision: 0.8624 - Recall: 0.9260 - accuracy: 0.8947 - loss: 0.2919 - val_AUC: 0.9636 - val_Precision: 0.9592 - val_Recall: 0.2327 - val_accuracy: 0.6349 - val_loss: 0.5140\n",
      "Epoch 8/10\n",
      "\u001B[1m54/54\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 49ms/step - AUC: 0.9576 - Precision: 0.8795 - Recall: 0.9360 - accuracy: 0.9086 - loss: 0.2821 - val_AUC: 0.9583 - val_Precision: 0.8571 - val_Recall: 0.0297 - val_accuracy: 0.5419 - val_loss: 0.5818\n",
      "Epoch 9/10\n",
      "\u001B[1m54/54\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 49ms/step - AUC: 0.9577 - Precision: 0.8632 - Recall: 0.9258 - accuracy: 0.8942 - loss: 0.2752 - val_AUC: 0.9586 - val_Precision: 0.9545 - val_Recall: 0.1040 - val_accuracy: 0.5767 - val_loss: 0.5505\n",
      "Epoch 10/10\n",
      "\u001B[1m54/54\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 49ms/step - AUC: 0.9629 - Precision: 0.8831 - Recall: 0.9300 - accuracy: 0.9076 - loss: 0.2565 - val_AUC: 0.9608 - val_Precision: 0.9500 - val_Recall: 0.0941 - val_accuracy: 0.5721 - val_loss: 0.5926\n",
      "\u001B[1m68/68\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 30ms/step - AUC: 0.9685 - Precision: 0.9612 - Recall: 0.1112 - accuracy: 0.5705 - loss: 0.6034\n",
      "Test Results - Loss: 0.597248375415802, Accuracy: 0.577602207660675\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The following is the assignment description for the followning section, hidden from display\n",
    "### 6. Fine-tune the system\n",
    "1. Fine-tune the hyperparameters\n",
    "2. Once you are confident about your final model, measure its performance on the test set to estimate the generalisation error\n",
    "<!-- [//] # TODO: Optimizing the chosen models\n",
    "     [//] # TODO: What is each model predicting and how can we visualize that?\n",
    "-->\n",
    "# 6. Fine-tune the system\n",
    "With the previously acquired knowledge, Convolutional Networks outperform the\n",
    "simpler Feedforward neural network.\n",
    "\n",
    "We will also focus on optimizing the parameters so that we obtain the best\n",
    "results possible"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:05:39.153111Z",
     "start_time": "2024-12-15T02:05:39.030775Z"
    }
   },
   "source": [
    "# build CNN model\n",
    "pokemon_predictor = tf.keras.Sequential([\n",
    "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "\n",
    "    layers.Rescaling(1./255),\n",
    "\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "], name='Pokemon_Predictor')\n",
    "\n",
    "# compile model\n",
    "pokemon_predictor.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'Precision', 'Recall', 'AUC'],\n",
    "    jit_compile=True\n",
    ")\n",
    "\n",
    "# inspect model\n",
    "pokemon_predictor.summary()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"Pokemon_Predictor\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Pokemon_Predictor\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling_3 (\u001B[38;5;33mRescaling\u001B[0m)         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m3\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │         \u001B[38;5;34m1,792\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │           \u001B[38;5;34m256\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │        \u001B[38;5;34m73,856\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m, \u001B[38;5;34m32\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m, \u001B[38;5;34m32\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │           \u001B[38;5;34m512\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m, \u001B[38;5;34m32\u001B[0m, \u001B[38;5;34m256\u001B[0m)    │       \u001B[38;5;34m295,168\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m, \u001B[38;5;34m16\u001B[0m, \u001B[38;5;34m256\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m, \u001B[38;5;34m16\u001B[0m, \u001B[38;5;34m256\u001B[0m)    │         \u001B[38;5;34m1,024\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │        \u001B[38;5;34m65,792\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │           \u001B[38;5;34m257\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m438,657\u001B[0m (1.67 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">438,657</span> (1.67 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m437,761\u001B[0m (1.67 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">437,761</span> (1.67 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m896\u001B[0m (3.50 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:06:28.467130Z",
     "start_time": "2024-12-15T02:05:39.203474Z"
    }
   },
   "source": [
    "# training loop\n",
    "pokemon_predictor.fit(\n",
    "    train_ds,\n",
    "    epochs=25,\n",
    "    steps_per_epoch=2000 // BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# evaluate on test set\n",
    "test_loss, test_acc = pokemon_predictor.evaluate(test_ds)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:05:51.452521: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_5', 4 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m54/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m1s\u001B[0m 219ms/step - AUC: 0.8697 - Precision: 0.7554 - Recall: 0.8383 - accuracy: 0.7922 - loss: 0.4642"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:06:03.095719: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_7', 4 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-12-15 03:06:03.126255: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17281436893529773292\n",
      "/home/ieris19/.conda/envs/MachineLearning/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 292ms/step - AUC: 0.8736 - Precision: 0.7586 - Recall: 0.8435 - accuracy: 0.7962 - loss: 0.4575 - val_AUC: 0.7311 - val_Precision: 0.4698 - val_Recall: 1.0000 - val_accuracy: 0.4698 - val_loss: 0.8390\n",
      "Epoch 2/25\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 64ms/step - AUC: 0.9621 - Precision: 0.8734 - Recall: 0.9511 - accuracy: 0.9105 - loss: 0.2592 - val_AUC: 0.6448 - val_Precision: 0.4698 - val_Recall: 1.0000 - val_accuracy: 0.4698 - val_loss: 1.0729\n",
      "Epoch 3/25\n",
      "\u001B[1m52/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 72ms/step - AUC: 0.9705 - Precision: 0.8777 - Recall: 0.9459 - accuracy: 0.9126 - loss: 0.2343"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:06:17.086196: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17281436893529773292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 64ms/step - AUC: 0.9703 - Precision: 0.8777 - Recall: 0.9459 - accuracy: 0.9125 - loss: 0.2341 - val_AUC: 0.8557 - val_Precision: 0.4698 - val_Recall: 1.0000 - val_accuracy: 0.4698 - val_loss: 1.1146\n",
      "Epoch 4/25\n",
      "\u001B[1m54/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - AUC: 0.9722 - Precision: 0.8857 - Recall: 0.9530 - accuracy: 0.9182 - loss: 0.2159"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:06:21.107189: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17281436893529773292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 64ms/step - AUC: 0.9723 - Precision: 0.8864 - Recall: 0.9534 - accuracy: 0.9188 - loss: 0.2151 - val_AUC: 0.8922 - val_Precision: 0.4698 - val_Recall: 1.0000 - val_accuracy: 0.4698 - val_loss: 0.8462\n",
      "Epoch 5/25\n",
      "\u001B[1m54/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - AUC: 0.9777 - Precision: 0.9008 - Recall: 0.9419 - accuracy: 0.9217 - loss: 0.1950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:06:25.092029: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 12005889478531810639\n",
      "2024-12-15 03:06:25.094298: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17281436893529773292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 64ms/step - AUC: 0.9777 - Precision: 0.9007 - Recall: 0.9427 - accuracy: 0.9220 - loss: 0.1951 - val_AUC: 0.9783 - val_Precision: 0.4988 - val_Recall: 1.0000 - val_accuracy: 0.5279 - val_loss: 0.6488\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001B[1m68/68\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 44ms/step - AUC: 0.7237 - Precision: 0.4855 - Recall: 1.0000 - accuracy: 0.4855 - loss: 0.8235\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 11\u001B[0m\n\u001B[1;32m      2\u001B[0m pokemon_predictor\u001B[38;5;241m.\u001B[39mfit(\n\u001B[1;32m      3\u001B[0m     train_ds,\n\u001B[1;32m      4\u001B[0m     epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m25\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39m[early_stopping]\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# evaluate on test set\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m test_loss, test_acc \u001B[38;5;241m=\u001B[39m pokemon_predictor\u001B[38;5;241m.\u001B[39mevaluate(test_ds)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTest accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Present your solution\n",
    "1. Document what you have done\n",
    "2. Create a nice 15 minute video presentation with slides\n",
    "    * Make sure you highlight the big picture first\n",
    "3. Explain why your solution achieves the business objective\n",
    "4. Don't forget to present interesting points you noticed along the way:\n",
    "    * Describe what worked and what did not\n",
    "    * List your assumptions and you model's limitations\n",
    "5. Ensure your key findings are communicated through nice visualisations or easy-to-remember statements (e.g. \"the median income is the number-one predictor of housing prices\")\n",
    "6. Upload the presentation to some online platform, e.g. YouTube or Vimeo, and supply a link to the video in the notebook."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_single_image(img, predictor):\n",
    "    x = np.expand_dims(img, axis=0)\n",
    "    return predictor.predict(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "__pokemon_image = plt.imread(os.path.join(pokemon_images_dir, '25.png'))\n",
    "__pokemon_resized = plt.imread(os.path.join(pkm_output, '25.png'))\n",
    "__digimon_image = plt.imread(os.path.join(digimon_images_dir, 'Agumon.jpg'))\n",
    "__digimon_resized = plt.imread(os.path.join(dmn_output, 'Agumon.png'))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.grid(False)\n",
    "plt.imshow(__pokemon_image)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.grid(False)\n",
    "plt.title(predict_single_image(__pokemon_image, pokemon_predictor))\n",
    "plt.imshow(__pokemon_resized)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.grid(False)\n",
    "plt.imshow(__digimon_image)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.grid(False)\n",
    "plt.title(predict_single_image(__digimon_image, pokemon_predictor))\n",
    "plt.imshow(__digimon_resized)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//] # TODO: Documentation\n",
    "## Final Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Géron, A. 2017, *Hands-On Machine Learning with Scikit-Learn and Tensorflow*, Appendix B, O'Reilly Media, Inc., Sebastopol."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
