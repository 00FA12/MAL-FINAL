{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Group Assignment & Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__You should be able to start up on this exercise after Lecture 1.__\n",
    "\n",
    "*This exercise must be a group effort. That means everyone must participate in the assignment.*\n",
    "\n",
    "In this assignment you will solve a data science problem end-to-end, pretending to be recently hired data scientists in\n",
    "a company. To help you get started, we've prepared a checklist to guide you through the project. Here are the main steps\n",
    "that you will go through:\n",
    "\n",
    "1. Frame the problem and look at the big picture\n",
    "2. Get the data\n",
    "3. Explore and visualise the data to gain insights\n",
    "4. Prepare the data to better expose the underlying data patterns to machine learning algorithms\n",
    "5. Explore many different models and short-list the best ones\n",
    "6. Fine-tune your models\n",
    "7. Present your solution (video presentation) \n",
    "\n",
    "In each step we list a set of questions that one should have in mind when undertaking a data science project. The list\n",
    "is not meant to be exhaustive, but does contain a selection of the most important questions to ask. We will be available\n",
    "to provide assistance with each of the steps, and will allocate some part of each lesson towards working on the projects.\n",
    "\n",
    "Your group must submit a _**single**_ Jupyter notebook, structured in terms of the first 6 sections listed above\n",
    "(the seventh will be a video uploaded to some streaming platform, e.g. YouTube, Vimeo, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:35:01.658714Z",
     "start_time": "2024-12-12T15:35:01.654988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the necessary libraries for this project\n",
    "# Standard Library modules\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# Dependencies\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Analysis: Frame the problem and look at the big picture\n",
    "1. Find a problem/task that everyone in the group finds interesting\n",
    "2. Define the objective in business terms\n",
    "3. How should you frame the problem (supervised/unsupervised etc.)?\n",
    "4. How should performance be measured?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analysis: Frame the problem and look at the big picture\n",
    "TODO: Write a description of the problem and the business objective. Define the problem in business terms.\n",
    "\n",
    "The following project will consist of a machine-learning system that identifies and classifies Pokemon from images,\n",
    "allowing the users to:\n",
    "\n",
    "##### 1. Verify if the image displays a Pokemon:\n",
    "Determine if an uploaded image contains a Pokemon or not, so that the identification of a Pokemon can be automated,\n",
    "saving time with large datasets.\n",
    "\n",
    "##### 2. Type classification:\n",
    "Additionally, the system should be able to predict the displayed Pokemon type/s'. This would allow the end user to achieve\n",
    "greater personalization, reaching a point in which it could recommend strategies, based on the opponent's Pokemon team.\n",
    "\n",
    "\n",
    "#### Framing the problem\n",
    "Supervised learning is the most appropiate approach, since a complete dataset can be supplied. Moreover, since the data\n",
    "is not continuous, classification will be used:\n",
    "\n",
    "- Is it a Pokemon?\n",
    "To answer this question, Binary classification we will need to implement a machine learning model that can perform\n",
    "image classification. In order to train this model, a dataset containing images of Pokemon and Digimon will be provided.\n",
    "To measure the performance of this model, there are a few metrics to take into account:\n",
    "  - Accuracy: Measurement of how many guesses are correct. Above 90% should be an acceptable value.\n",
    "  - Precision: Fraction of the predicted Pokemon that are actually Pokemon:\n",
    "  $$ \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} $$\n",
    "  - Recall: Fraction of actual Pokémon images correctly identified:\n",
    "  $$\\text{Recall}= \\frac {\\text{True Positives}}{\\text{True Positives + False Negatives}} $$\n",
    "  - F1 score: since the dataset is imbalanced, a harmonic mean of precision and recall is also useful:\n",
    "  $$\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "- Which is this species' type?\n",
    "To answer this question, we will need a machine learning model capable of labelling the images accordingly, using many\n",
    "different labels, as the expected output could consist of multiple elements, since some Pokemon species possess 2 types.\n",
    "\n",
    "To measure the performance of this model, there are a few metrics to take into account:\n",
    "  - Hamming Loss: Fraction of incorrectly predicted labels (either false positive or false negative):\n",
    "  $$\\text{Hamming Loss} = \\frac{\\text{Number of Incorrect Labels}}{\\text{Total Labels}}$$\n",
    "  - Precision: Fraction of predicted types that are correct (averaged across labels).\n",
    "  - Recall: Fraction of actual types correctly identified:\n",
    "  $$\\text{Recall}= \\frac {\\text{True Positives}}{\\text{True Positives + False Negatives}} $$\n",
    "  - F1 score: since the dataset is imbalanced, a harmonic mean of precision and recall is also useful:\n",
    "  $$\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "  - Mean Average Precision (mAP): Average precision computed for each type, capturing how well the model ranks true positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the data\n",
    "1. Find and document where you can get the data from\n",
    "2. Get the data\n",
    "3. Check the size and type of data (time series, geographical etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the data\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "To train the model for Pokémon classification, two datasets have been selected\n",
    "to cover the primary tasks: identifying Pokémon and distinguishing them from\n",
    "non-Pokémon images. These datasets provide a robust foundation for the project.\n",
    "\n",
    "- The PokeAPI dataset is an extensive dataset containing all sorts of information\n",
    "about the Pokemon universe, including information on all Pokemon species, images\n",
    "and other information such as moves, abilities, and locations. The dataset is\n",
    "available on GitHub [here](https://github.com/PokeAPI/pokeapi.git) and the\n",
    "corresponding submodules.\n",
    "\n",
    "- The Digimon dataset is a collection of images scraped from the\n",
    "[Wikimon.net](https://wikimon.net/Visual_List_of_Digimon) website and contains\n",
    "about 1000 images of Digimon species and variations, since this only serves as\n",
    "negative examples of Pokemon, we only need their images and none of the additional\n",
    "information about them.\n",
    "\n",
    "The web scrapper used to gather the Digimon data can be found on GitHub\n",
    "[here](https://github.com/lorenzo-stacchio/Digimon_Dataset), although there\n",
    "exists a Google Drive link with the data already gathered, which can be found\n",
    "[here](https://drive.google.com/drive/folders/1tmcdsoX67NvmAgtmGJgo6kb3N6SlJeLu?usp=share_link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explore the data\n",
    "1. Create a copy of the data for explorations (sampling it down to a manageable size if necessary)\n",
    "2. Create a Jupyter notebook to keep a record of your data exploration\n",
    "3. Study each feature and its characteristics:\n",
    "    * Name\n",
    "    * Type (categorical, int/float, bounded/unbounded, text, structured, etc)\n",
    "    * Percentage of missing values\n",
    "    * Check for outliers, rounding errors etc\n",
    "4. For supervised learning tasks, identify the target(s)\n",
    "5. Visualise the data\n",
    "6. Study the correlations between features\n",
    "7. Identify the promising transformations you may want to apply (e.g. convert skewed targets to normal via a log transformation)\n",
    "8. Document what you have learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "- PokeAPI Dataset:\n",
    "    - The dataset contains images for all Pokemon species, one for each game\n",
    "    they have been in among many alternate forms and color variations.\n",
    "    - The dataset also includes CSV files with information about Pokemon, where\n",
    "    to find them, their moves and abilities, etc...\n",
    "- Digimon Dataset\n",
    "    - The dataset contains images scraped from the Digimon wiki, this includes\n",
    "    roughly about a thousand images of Digimon species and variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_properties(path):\n",
    "    props = {}\n",
    "    property_regex = re.compile(r'#{0}(.+)[:=]([^\\n\\r#]+)#?')\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            match = property_regex.match(line)\n",
    "            if match:\n",
    "                key = match.group(1).strip()\n",
    "                value = match.group(2).strip()\n",
    "                #print('Found property: {}={}'.format(key, value))\n",
    "                props[key] = value\n",
    "    return props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:35:01.764187Z",
     "start_time": "2024-12-12T15:35:01.758302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pokeAPI-data': '\"\"',\n",
       " 'pokeAPI-sprites': '\"\"',\n",
       " 'digimon-images': '\"\"',\n",
       " 'dataset-dir': './Dataset'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "props = read_properties('paths.env')\n",
    "\n",
    "pokeAPI_data_repo = props['pokeAPI-data']\n",
    "pokeAPI_sprites_repo = props['pokeAPI-sprites']\n",
    "digimon_datasource = props['digimon-images']\n",
    "\n",
    "output_dir = props['dataset-dir']\n",
    "display(props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:35:01.807097Z",
     "start_time": "2024-12-12T15:35:01.804028Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using __ as a prefix to clarify that variables are not meant to be used later\n",
    "# PokeAPI csv data\n",
    "__pokeAPI_data_root = os.path.join(pokeAPI_data_repo, 'data/v2/csv')\n",
    "# PokeAPI sprites folder\n",
    "__pokeAPI_sprites_folder = os.path.join(pokeAPI_sprites_repo, 'sprites')\n",
    "# Digimon images folder\n",
    "__digimon_images_folder = os.path.join(digimon_datasource, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:35:01.860296Z",
     "start_time": "2024-12-12T15:35:01.853600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for data...\n",
      "You must download the PokeAPI data first from the following git: https://github.com/PokeAPI/pokeapi.git\n",
      "You must download the PokeAPI sprites first from the following git: https://github.com/PokeAPI/sprites.git\n",
      "You must download the Digimon data first from the following Google Drive: https://drive.google.com/drive/folders/1tmcdsoX67NvmAgtmGJgo6kb3N6SlJeLu?usp=share_link\n",
      "You must download the following sources: PokeAPI data, PokeAPI sprites, Digimon images\n"
     ]
    }
   ],
   "source": [
    "print('Checking for data...')\n",
    "__pokeAPI_data_url = 'https://github.com/PokeAPI/pokeapi.git'\n",
    "__pokeAPI_sprites_url = 'https://github.com/PokeAPI/sprites.git'\n",
    "__digimon_data_url = 'https://drive.google.com/drive/folders/1tmcdsoX67NvmAgtmGJgo6kb3N6SlJeLu?usp=share_link'\n",
    "\n",
    "__missing_sources = []\n",
    "if not os.path.exists(__pokeAPI_data_root):\n",
    "    print('You must download the PokeAPI data first from the following git: {}'.format(__pokeAPI_data_url))\n",
    "    __missing_sources.append('PokeAPI data')\n",
    "else:\n",
    "    print('Retrieving PokeAPI data from: {}'.format(__pokeAPI_data_root))\n",
    "\n",
    "if not os.path.exists(__pokeAPI_sprites_folder):\n",
    "    print('You must download the PokeAPI sprites first from the following git: {}'.format(__pokeAPI_sprites_url))\n",
    "    __missing_sources.append('PokeAPI sprites')\n",
    "else:\n",
    "    print('Retrieving PokeAPI sprites from: {}'.format(__pokeAPI_sprites_folder))\n",
    "\n",
    "if not os.path.exists(__digimon_images_folder):\n",
    "    print('You must download the Digimon data first from the following Google Drive: {}'.format(__digimon_data_url))\n",
    "    __missing_sources.append('Digimon images')\n",
    "else:\n",
    "    print('Retrieving Digimon images from: {}'.format(__digimon_images_folder))\n",
    "\n",
    "if len(__missing_sources) > 1:\n",
    "    print('You must download the following sources: {}'.format(', '.join(__missing_sources)))\n",
    "    # raise FileNotFoundError('Missing data sources')\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:35:01.911088Z",
     "start_time": "2024-12-12T15:35:01.906966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has already been generated, please delete to regenerate\n"
     ]
    }
   ],
   "source": [
    "# Verify the datasource hasn't generated earlier\n",
    "dataset_dir = os.path.join(output_dir, 'dataset')\n",
    "dataset_path = os.path.join(dataset_dir, 'pokemon.csv')\n",
    "generate_dataset = True\n",
    "if os.path.exists(dataset_path):\n",
    "    generate_dataset = False\n",
    "    print('The dataset has already been generated, please delete to regenerate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:35:01.961744Z",
     "start_time": "2024-12-12T15:35:01.958427Z"
    }
   },
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    original_pokemon = pd.read_csv(os.path.join(__pokeAPI_data_root, 'pokemon.csv'))\n",
    "    original_types = pd.read_csv(os.path.join(__pokeAPI_data_root, 'types.csv'))\n",
    "    original_pokemon_types = pd.read_csv(os.path.join(__pokeAPI_data_root, 'pokemon_types.csv'))\n",
    "    return original_pokemon, original_types, original_pokemon_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming the data\n",
    "\n",
    "The PokeAPI dataset is quite extensive, it contains all Pokemon (currently 1025)\n",
    "and also many alternative form information. This information is assigned an ID\n",
    "that is greater than 10000 so that it does not interfere with the original Pokemon\n",
    "IDs, as such, we must filter out all IDs greater than 10000.\n",
    "\n",
    "Additionally, we will remove a lot of the columns that are not relevant to our\n",
    "current analysis, such as the foreign keys pointing to relationships outside our\n",
    "scope and some of the data irrelevant to us such as height and weight.\n",
    "\n",
    "There's also some types that are not relevant to our analysis, because they are\n",
    "only used for specific mechanics in the games and do not represent a Pokemon's\n",
    "type, such as the \"shadow\" type. We will remove these types from the dataset,\n",
    "thankfully, the same as Pokemon IDs apply, these types have IDs greater than 10000.\n",
    "\n",
    "We will also rename some of the columns to make them more readable and to avoid\n",
    "confusion later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:35:02.011457Z",
     "start_time": "2024-12-12T15:35:02.007335Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_source_datasets(original_pokemon, original_types, original_pokemon_types):\n",
    "    id_cutoff = 10000\n",
    "\n",
    "    pokemon_types = original_pokemon_types[original_pokemon_types['pokemon_id'] < id_cutoff]\n",
    "    pokemon = original_pokemon[original_pokemon['id'] < id_cutoff]\n",
    "    types = original_types[original_types['id'] < id_cutoff]\n",
    "    types = types.drop(columns=['damage_class_id', 'generation_id'])\n",
    "    pokemon = pokemon.drop(columns=['species_id', 'height', 'weight', 'base_experience', 'order', 'is_default'])\n",
    "    pokemon = pokemon.rename(columns={'identifier': 'name'})\n",
    "    types = types.rename(columns={'identifier': 'type_label'})\n",
    "    return pokemon, types, pokemon_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:35:02.060896Z",
     "start_time": "2024-12-12T15:35:02.055813Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_dataset(pokemon, types, pokemon_types):\n",
    "    merged = pokemon_types.merge(types, left_on='type_id', right_on='id').drop(columns=['id'])\n",
    "    merged = merged.rename(columns={'identifier': 'type', 'slot': 'type_slot'})\n",
    "    merged = pokemon.merge(merged, left_on='id', right_on='pokemon_id').drop(columns=['id'])\n",
    "    dataset_target = os.path.join(dataset_dir, 'pokemon.csv')\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "    pokemon_merged.to_csv(dataset_target, index=False)\n",
    "    print('Pokemon data saved to {} for saving'.format(dataset_target))\n",
    "    return pokemon_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:35:02.122063Z",
     "start_time": "2024-12-12T15:35:02.107682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset from ./Dataset/dataset/pokemon.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pokemon_id</th>\n",
       "      <th>type_id</th>\n",
       "      <th>type_slot</th>\n",
       "      <th>type_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bulbasaur</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bulbasaur</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ivysaur</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ivysaur</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>venusaur</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>iron-crown</td>\n",
       "      <td>1023</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>iron-crown</td>\n",
       "      <td>1023</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>psychic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>terapagos</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>pecharunt</td>\n",
       "      <td>1025</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>pecharunt</td>\n",
       "      <td>1025</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>ghost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1551 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  pokemon_id  type_id  type_slot type_label\n",
       "0      bulbasaur           1       12          1      grass\n",
       "1      bulbasaur           1        4          2     poison\n",
       "2        ivysaur           2       12          1      grass\n",
       "3        ivysaur           2        4          2     poison\n",
       "4       venusaur           3       12          1      grass\n",
       "...          ...         ...      ...        ...        ...\n",
       "1546  iron-crown        1023        9          1      steel\n",
       "1547  iron-crown        1023       14          2    psychic\n",
       "1548   terapagos        1024        1          1     normal\n",
       "1549   pecharunt        1025        4          1     poison\n",
       "1550   pecharunt        1025        8          2      ghost\n",
       "\n",
       "[1551 rows x 5 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate or read the dataset based on the previous checks\n",
    "if generate_dataset:\n",
    "    pokemon_merged = merge_dataset(*clean_source_datasets(*import_data()))\n",
    "else:\n",
    "    print('Reading dataset from {}'.format(dataset_path))\n",
    "    pokemon_merged = pd.read_csv(dataset_path)\n",
    "pokemon_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:35:02.195844Z",
     "start_time": "2024-12-12T15:35:02.191692Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if images have already been extracted:\n",
    "images_dir = os.path.join(output_dir, 'images')\n",
    "extract_images = {}\n",
    "pokemon_images_dir = os.path.join(images_dir, 'pokemon')\n",
    "digimon_images_dir = os.path.join(images_dir, 'digimon')\n",
    "extract_images['pokemon'] = not os.path.exists(pokemon_images_dir)\n",
    "extract_images['digimon'] = not os.path.exists(digimon_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:35:02.236124Z",
     "start_time": "2024-12-12T15:35:02.231486Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_pokemon_images():\n",
    "    source_images = os.path.join(__pokeAPI_sprites_folder, 'pokemon/other/official-artwork')\n",
    "    poke_image_files = os.listdir(source_images)\n",
    "    print('Preparing {} Pokemon images...'.format(len(poke_image_files)))\n",
    "    os.makedirs(pokemon_images_dir)\n",
    "    # Like the data, images with ID higher than 10000 are not relevant\n",
    "    valid_image_pattern = re.compile(r'^[0-9]{1,4}\\.png')\n",
    "    for file in poke_image_files:\n",
    "        if valid_image_pattern.match(file):\n",
    "            source = os.path.join(source_images, file)\n",
    "            target = os.path.join(pokemon_images_dir, file)\n",
    "            shutil.copy(source, target)\n",
    "            print('Copied {} to {}'.format(file, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:35:02.346294Z",
     "start_time": "2024-12-12T15:35:02.341720Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_digimon_images():\n",
    "    digimon_image_files = os.listdir(__digimon_images_folder)\n",
    "    print('Preparing {} Digimon images...'.format(len(digimon_image_files)))\n",
    "    os.makedirs(digimon_images_dir)\n",
    "    for file in digimon_image_files:\n",
    "        source = os.path.join(__digimon_images_folder, file)\n",
    "        target = os.path.join(digimon_images_dir, file)\n",
    "        shutil.copy(source, target)\n",
    "        print('Copied file\\n\\tfrom: {}\\n\\tto: {}'.format(file, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pokemon images already extracted\n",
      "Digimon images already extracted\n"
     ]
    }
   ],
   "source": [
    "if extract_images['pokemon']:\n",
    "    extract_pokemon_images()\n",
    "else:\n",
    "    print('Pokemon images already extracted')\n",
    "if extract_images['digimon']:\n",
    "    extract_digimon_images()\n",
    "else:\n",
    "    print('Digimon images already extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add some visualization here to show some images and maybe some graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare the data\n",
    "Notes:\n",
    "* Work on copies of the data (keep the original dataset intact).\n",
    "* Write functions for all data transformations you apply, for three reasons:\n",
    "    * So you can easily prepare the data the next time you run your code\n",
    "    * So you can apply these transformations in future projects\n",
    "    * To clean and prepare the test set\n",
    "\n",
    "1. Data cleaning:\n",
    "    * Fix or remove outliers (or keep them)\n",
    "    * Fill in missing values (e.g. with zero, mean, median, regression ...) or drop their rows (or columns)\n",
    "2. Feature selection (optional):\n",
    "    * Drop the features that provide no useful information for the task (e.g. a customer ID is usually useless for modelling).\n",
    "3. Feature engineering, where appropriate:\n",
    "    * Discretize continuous features\n",
    "    * Use one-hot encoding if/when relevant\n",
    "    * Add promising transformations of features (e.g. $\\log(x)$, $\\sqrt{x}$, $x^2$, etc)\n",
    "    * Aggregate features into promising new features\n",
    "4. Feature scaling: standardise or normalise features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This is where the images must be processed to be used in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output folder ./Dataset/images/new is not empty.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "output_folder = f\"{images_dir}/new\"  # Replace with your output folder path\n",
    "new_size = (128, 128)  # Set the desired size (width, height)\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def resize_images(input_folder, output_folder, new_size):\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        file_extension = os.path.splitext(filename)[1].lower()\n",
    "        print(file_extension)\n",
    "        try:\n",
    "            with Image.open(input_path) as img:\n",
    "                img.thumbnail(new_size)  # Resizes while maintaining aspect ratio\n",
    "                if file_extension in ['.jpg', '.jpeg']:\n",
    "                    img = img.convert('RGB')  # Ensure JPEG images are in RGB mode\n",
    "                    output_path = os.path.splitext(output_path)[0] + '.png'  # Change extension to .png\n",
    "                    img.save(output_path, 'PNG')\n",
    "                else:\n",
    "                    img.save(output_path, file_extension[1:].upper())\n",
    "                print(f\"Resized and saved: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {filename}: {e}\")\n",
    "# Check if output_folder is empty\n",
    "if not os.listdir(f\"{output_folder}/pokemon\") and not os.listdir(f\"{output_folder}/digimon\"):\n",
    "    print(f\"The output folder {output_folder} is empty.\")\n",
    "    resize_images(pokemon_images_dir, f\"{output_folder}/pokemon\", new_size)\n",
    "    resize_images(digimon_images_dir, f\"{output_folder}/digimon\", new_size)\n",
    "else:\n",
    "    print(f\"The output folder {output_folder} is not empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Short-list promising models\n",
    "We expect you to do some additional research and train at **least one model per team member**.\n",
    "\n",
    "1. Train mainly quick and dirty models from different categories (e.g. linear, SVM, Random Forests etc) using default parameters\n",
    "2. Measure and compare their performance\n",
    "3. Analyse the most significant variables for each algorithm\n",
    "4. Analyse the types of errors the models make\n",
    "5. Have a quick round of feature selection and engineering if necessary\n",
    "6. Have one or two more quick iterations of the five previous steps\n",
    "7. Short-list the top three to five most promising models, preferring models that make different types of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This is where we train two models !!! One for each task OR remove one of the tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset properties\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(path):\n",
    "    return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        path,\n",
    "        interpolation='area',\n",
    "        image_size=(IMG_SIZE, IMG_SIZE),\n",
    "        shuffle=False,\n",
    "        batch_size=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2152 files belonging to 2 classes.\n",
      "Found 2152 files belonging to 2 classes.\n",
      "Found 2152 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_ds = data_loader(f\"{images_dir}/new\").take(160).batch(BATCH_SIZE).prefetch(buffer_size=BUFFER_SIZE)\n",
    "val_ds = data_loader(f\"{images_dir}/new\").skip(160).take(200).batch(BATCH_SIZE).prefetch(buffer_size=BUFFER_SIZE)\n",
    "test_ds = data_loader(f\"{images_dir}/new\").skip(180).take(200).batch(BATCH_SIZE).prefetch(buffer_size=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember class names\n",
    "class_names = ['digimon', 'pokemon']\n",
    "\n",
    "# optimize performance\n",
    "train_ds = train_ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49152</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">25,166,336</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling_5 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49152\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m25,166,336\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,297,921</span> (96.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,297,921\u001b[0m (96.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,297,921</span> (96.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,297,921\u001b[0m (96.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build model\n",
    "fnn = models.Sequential([\n",
    "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)), # 3 channels for RGB\n",
    "    layers.Rescaling(1./255), # normalize pixel values to [0, 1]\n",
    "    layers.Flatten(), # flatten the 128x128x3 input images\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.125),\n",
    "    layers.Dense(1, activation='sigmoid') # output layer, binary classification\n",
    "])\n",
    "\n",
    "# compile model\n",
    "fnn.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    "    jit_compile=True\n",
    ")\n",
    "\n",
    "# inspect model\n",
    "fnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, None, 128, 128, 3), dtype=float32). Expected shape (None, 128, 128, 3), but input has incompatible shape (None, None, 128, 128, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, None, 128, 128, 3), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m      3\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      5\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m training \u001b[38;5;241m=\u001b[39m \u001b[43mfnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# evaluate on test set\u001b[39;00m\n\u001b[1;32m     19\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m fnn\u001b[38;5;241m.\u001b[39mevaluate(test_ds)\n",
      "File \u001b[0;32m~/Documents/VIA/6th/MAL/Final Group Project/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/VIA/6th/MAL/Final Group Project/.venv/lib/python3.12/site-packages/keras/src/models/functional.py:273\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    271\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    272\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, None, 128, 128, 3), dtype=float32). Expected shape (None, 128, 128, 3), but input has incompatible shape (None, None, 128, 128, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, None, 128, 128, 3), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# stop when val_accuracy doesnt improve\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# training loop\n",
    "training = fnn.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=25,\n",
    "    steps_per_epoch=2000 // BATCH_SIZE,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# evaluate on test set\n",
    "test_loss, test_acc = fnn.evaluate(test_ds)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Fine-tune the system\n",
    "1. Fine-tune the hyperparameters\n",
    "2. Once you are confident about your final model, measure its performance on the test set to estimate the generalisation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimizing the chosen models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Present your solution\n",
    "1. Document what you have done\n",
    "2. Create a nice 15 minute video presentation with slides\n",
    "    * Make sure you highlight the big picture first\n",
    "3. Explain why your solution achieves the business objective\n",
    "4. Don't forget to present interesting points you noticed along the way:\n",
    "    * Describe what worked and what did not\n",
    "    * List your assumptions and you model's limitations\n",
    "5. Ensure your key findings are communicated through nice visualisations or easy-to-remember statements (e.g. \"the median income is the number-one predictor of housing prices\")\n",
    "6. Upload the presentation to some online platform, e.g. YouTube or Vimeo, and supply a link to the video in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Géron, A. 2017, *Hands-On Machine Learning with Scikit-Learn and Tensorflow*, Appendix B, O'Reilly Media, Inc., Sebastopol."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
